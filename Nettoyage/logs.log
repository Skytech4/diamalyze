2025-05-05 00:26:00,868:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 00:26:00,868:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 00:26:00,868:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 00:26:00,868:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 00:26:17,809:INFO:PyCaret ClassificationExperiment
2025-05-05 00:26:17,811:INFO:Logging name: clf-default-name
2025-05-05 00:26:17,812:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-05 00:26:17,815:INFO:version 3.3.2
2025-05-05 00:26:17,816:INFO:Initializing setup()
2025-05-05 00:26:17,816:INFO:self.USI: c630
2025-05-05 00:26:17,816:INFO:self._variable_keys: {'X_test', '_available_plots', 'html_param', 'data', 'idx', 'y_test', 'fold_generator', 'gpu_param', 'pipeline', 'n_jobs_param', 'target_param', 'is_multiclass', 'y_train', 'logging_param', 'log_plots_param', 'seed', 'fold_groups_param', 'memory', 'X', 'exp_name_log', 'exp_id', '_ml_usecase', 'gpu_n_jobs_param', 'USI', 'fix_imbalance', 'y', 'fold_shuffle_param', 'X_train'}
2025-05-05 00:26:17,817:INFO:Checking environment
2025-05-05 00:26:17,817:INFO:python_version: 3.10.11
2025-05-05 00:26:17,817:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-05 00:26:17,817:INFO:machine: AMD64
2025-05-05 00:26:17,818:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-05 00:26:17,818:INFO:Memory: svmem(total=8482373632, available=1943805952, percent=77.1, used=6538567680, free=1943805952)
2025-05-05 00:26:17,818:INFO:Physical Core: 2
2025-05-05 00:26:17,818:INFO:Logical Core: 4
2025-05-05 00:26:17,818:INFO:Checking libraries
2025-05-05 00:26:17,818:INFO:System:
2025-05-05 00:26:17,818:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-05 00:26:17,818:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-05 00:26:17,818:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-05 00:26:17,818:INFO:PyCaret required dependencies:
2025-05-05 00:26:19,467:INFO:                 pip: 25.0.1
2025-05-05 00:26:19,467:INFO:          setuptools: 65.5.0
2025-05-05 00:26:19,467:INFO:             pycaret: 3.3.2
2025-05-05 00:26:19,467:INFO:             IPython: 8.34.0
2025-05-05 00:26:19,467:INFO:          ipywidgets: 8.1.5
2025-05-05 00:26:19,467:INFO:                tqdm: 4.67.1
2025-05-05 00:26:19,467:INFO:               numpy: 1.26.4
2025-05-05 00:26:19,467:INFO:              pandas: 2.1.4
2025-05-05 00:26:19,467:INFO:              jinja2: 3.1.6
2025-05-05 00:26:19,467:INFO:               scipy: 1.11.4
2025-05-05 00:26:19,467:INFO:              joblib: 1.3.2
2025-05-05 00:26:19,467:INFO:             sklearn: 1.4.2
2025-05-05 00:26:19,467:INFO:                pyod: 2.0.4
2025-05-05 00:26:19,467:INFO:            imblearn: 0.13.0
2025-05-05 00:26:19,467:INFO:   category_encoders: 2.7.0
2025-05-05 00:26:19,467:INFO:            lightgbm: 4.6.0
2025-05-05 00:26:19,467:INFO:               numba: 0.61.0
2025-05-05 00:26:19,467:INFO:            requests: 2.32.3
2025-05-05 00:26:19,467:INFO:          matplotlib: 3.7.5
2025-05-05 00:26:19,467:INFO:          scikitplot: 0.3.7
2025-05-05 00:26:19,467:INFO:         yellowbrick: 1.5
2025-05-05 00:26:19,467:INFO:              plotly: 5.24.1
2025-05-05 00:26:19,467:INFO:    plotly-resampler: Not installed
2025-05-05 00:26:19,467:INFO:             kaleido: 0.2.1
2025-05-05 00:26:19,467:INFO:           schemdraw: 0.15
2025-05-05 00:26:19,467:INFO:         statsmodels: 0.14.4
2025-05-05 00:26:19,467:INFO:              sktime: 0.26.0
2025-05-05 00:26:19,467:INFO:               tbats: 1.1.3
2025-05-05 00:26:19,467:INFO:            pmdarima: 2.0.4
2025-05-05 00:26:19,467:INFO:              psutil: 7.0.0
2025-05-05 00:26:19,467:INFO:          markupsafe: 3.0.2
2025-05-05 00:26:19,467:INFO:             pickle5: Not installed
2025-05-05 00:26:19,467:INFO:         cloudpickle: 3.1.1
2025-05-05 00:26:19,467:INFO:         deprecation: 2.1.0
2025-05-05 00:26:19,467:INFO:              xxhash: 3.5.0
2025-05-05 00:26:19,467:INFO:           wurlitzer: Not installed
2025-05-05 00:26:19,467:INFO:PyCaret optional dependencies:
2025-05-05 00:26:19,565:INFO:                shap: 0.44.1
2025-05-05 00:26:19,565:INFO:           interpret: 0.6.10
2025-05-05 00:26:19,565:INFO:                umap: 0.5.7
2025-05-05 00:26:19,565:INFO:     ydata_profiling: 4.16.1
2025-05-05 00:26:19,565:INFO:  explainerdashboard: 0.4.8
2025-05-05 00:26:19,565:INFO:             autoviz: Not installed
2025-05-05 00:26:19,565:INFO:           fairlearn: 0.7.0
2025-05-05 00:26:19,565:INFO:          deepchecks: Not installed
2025-05-05 00:26:19,565:INFO:             xgboost: Not installed
2025-05-05 00:26:19,565:INFO:            catboost: Not installed
2025-05-05 00:26:19,565:INFO:              kmodes: Not installed
2025-05-05 00:26:19,565:INFO:             mlxtend: Not installed
2025-05-05 00:26:19,565:INFO:       statsforecast: Not installed
2025-05-05 00:26:19,565:INFO:        tune_sklearn: Not installed
2025-05-05 00:26:19,565:INFO:                 ray: Not installed
2025-05-05 00:26:19,565:INFO:            hyperopt: Not installed
2025-05-05 00:26:19,565:INFO:              optuna: 4.2.1
2025-05-05 00:26:19,565:INFO:               skopt: Not installed
2025-05-05 00:26:19,565:INFO:              mlflow: Not installed
2025-05-05 00:26:19,565:INFO:              gradio: Not installed
2025-05-05 00:26:19,565:INFO:             fastapi: Not installed
2025-05-05 00:26:19,565:INFO:             uvicorn: Not installed
2025-05-05 00:26:19,565:INFO:              m2cgen: Not installed
2025-05-05 00:26:19,565:INFO:           evidently: Not installed
2025-05-05 00:26:19,565:INFO:               fugue: Not installed
2025-05-05 00:26:19,565:INFO:           streamlit: Not installed
2025-05-05 00:26:19,565:INFO:             prophet: Not installed
2025-05-05 00:26:19,565:INFO:None
2025-05-05 00:26:19,565:INFO:Set up data.
2025-05-05 00:26:19,706:INFO:Set up folding strategy.
2025-05-05 00:26:19,707:INFO:Set up train/test split.
2025-05-05 00:26:19,798:INFO:Set up index.
2025-05-05 00:26:19,814:INFO:Assigning column types.
2025-05-05 00:26:19,900:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-05 00:26:20,114:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 00:26:20,133:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 00:26:20,383:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:20,383:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:20,664:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 00:26:20,664:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 00:26:20,782:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:20,782:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:20,782:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-05 00:26:20,912:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 00:26:20,982:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:20,982:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:21,097:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 00:26:21,188:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:21,188:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:21,196:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-05 00:26:21,397:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:21,397:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:21,583:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:21,583:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:21,583:INFO:Preparing preprocessing pipeline...
2025-05-05 00:26:21,606:INFO:Set up simple imputation.
2025-05-05 00:26:21,606:INFO:Set up imbalanced handling.
2025-05-05 00:26:21,606:INFO:Set up feature normalization.
2025-05-05 00:26:22,864:INFO:Finished creating preprocessing pipeline.
2025-05-05 00:26:22,881:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'color', 'clarity',
                                             'depth', 'table', 'price', 'x',
                                             'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('ca...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-05 00:26:22,881:INFO:Creating final display dataframe.
2025-05-05 00:26:24,031:INFO:Setup _display_container:                     Description             Value
0                    Session id              6000
1                        Target               cut
2                   Target type        Multiclass
3           Original data shape       (53940, 10)
4        Transformed data shape       (96993, 10)
5   Transformed train set shape       (86205, 10)
6    Transformed test set shape       (10788, 10)
7              Numeric features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                Fix imbalance              True
13         Fix imbalance method             smote
14                    Normalize              True
15             Normalize method            minmax
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              c630
2025-05-05 00:26:24,397:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:24,398:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:24,614:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:24,614:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 00:26:24,614:INFO:setup() successfully completed in 7.4s...............
2025-05-05 00:28:03,620:INFO:Initializing compare_models()
2025-05-05 00:28:03,620:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-05-05 00:28:03,620:INFO:Checking exceptions
2025-05-05 00:28:03,703:INFO:Preparing display monitor
2025-05-05 00:28:03,850:INFO:Initializing Logistic Regression
2025-05-05 00:28:03,850:INFO:Total runtime is 0.0 minutes
2025-05-05 00:28:03,891:INFO:SubProcess create_model() called ==================================
2025-05-05 00:28:03,899:INFO:Initializing create_model()
2025-05-05 00:28:03,899:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:28:03,899:INFO:Checking exceptions
2025-05-05 00:28:03,899:INFO:Importing libraries
2025-05-05 00:28:03,899:INFO:Copying training dataset
2025-05-05 00:28:04,242:INFO:Defining folds
2025-05-05 00:28:04,242:INFO:Declaring metric variables
2025-05-05 00:28:04,276:INFO:Importing untrained model
2025-05-05 00:28:04,441:INFO:Logistic Regression Imported successfully
2025-05-05 00:28:04,665:INFO:Starting cross validation
2025-05-05 00:28:04,719:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:28:31,780:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:31,780:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:31,785:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:41,873:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:42,424:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:42,941:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:43,489:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:50,773:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:50,773:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:28:50,824:INFO:Calculating mean and std
2025-05-05 00:28:50,831:INFO:Creating metrics dataframe
2025-05-05 00:28:50,843:INFO:Uploading results into container
2025-05-05 00:28:50,846:INFO:Uploading model into container now
2025-05-05 00:28:50,849:INFO:_master_model_container: 1
2025-05-05 00:28:50,850:INFO:_display_container: 2
2025-05-05 00:28:50,852:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6000, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-05 00:28:50,857:INFO:create_model() successfully completed......................................
2025-05-05 00:28:51,091:INFO:SubProcess create_model() end ==================================
2025-05-05 00:28:51,091:INFO:Creating metrics dataframe
2025-05-05 00:28:51,187:INFO:Initializing K Neighbors Classifier
2025-05-05 00:28:51,187:INFO:Total runtime is 0.7889495452245077 minutes
2025-05-05 00:28:51,219:INFO:SubProcess create_model() called ==================================
2025-05-05 00:28:51,221:INFO:Initializing create_model()
2025-05-05 00:28:51,221:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:28:51,223:INFO:Checking exceptions
2025-05-05 00:28:51,224:INFO:Importing libraries
2025-05-05 00:28:51,225:INFO:Copying training dataset
2025-05-05 00:28:51,458:INFO:Defining folds
2025-05-05 00:28:51,458:INFO:Declaring metric variables
2025-05-05 00:28:51,489:INFO:Importing untrained model
2025-05-05 00:28:51,508:INFO:K Neighbors Classifier Imported successfully
2025-05-05 00:28:51,576:INFO:Starting cross validation
2025-05-05 00:28:51,581:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:29:03,841:INFO:Calculating mean and std
2025-05-05 00:29:03,846:INFO:Creating metrics dataframe
2025-05-05 00:29:03,875:INFO:Uploading results into container
2025-05-05 00:29:03,879:INFO:Uploading model into container now
2025-05-05 00:29:03,885:INFO:_master_model_container: 2
2025-05-05 00:29:03,896:INFO:_display_container: 2
2025-05-05 00:29:03,899:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-05-05 00:29:03,902:INFO:create_model() successfully completed......................................
2025-05-05 00:29:04,073:INFO:SubProcess create_model() end ==================================
2025-05-05 00:29:04,073:INFO:Creating metrics dataframe
2025-05-05 00:29:04,107:INFO:Initializing Naive Bayes
2025-05-05 00:29:04,108:INFO:Total runtime is 1.0042933305104573 minutes
2025-05-05 00:29:04,124:INFO:SubProcess create_model() called ==================================
2025-05-05 00:29:04,126:INFO:Initializing create_model()
2025-05-05 00:29:04,126:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:29:04,126:INFO:Checking exceptions
2025-05-05 00:29:04,127:INFO:Importing libraries
2025-05-05 00:29:04,128:INFO:Copying training dataset
2025-05-05 00:29:04,345:INFO:Defining folds
2025-05-05 00:29:04,345:INFO:Declaring metric variables
2025-05-05 00:29:04,357:INFO:Importing untrained model
2025-05-05 00:29:04,412:INFO:Naive Bayes Imported successfully
2025-05-05 00:29:04,461:INFO:Starting cross validation
2025-05-05 00:29:04,462:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:29:08,156:INFO:Calculating mean and std
2025-05-05 00:29:08,163:INFO:Creating metrics dataframe
2025-05-05 00:29:08,194:INFO:Uploading results into container
2025-05-05 00:29:08,200:INFO:Uploading model into container now
2025-05-05 00:29:08,202:INFO:_master_model_container: 3
2025-05-05 00:29:08,202:INFO:_display_container: 2
2025-05-05 00:29:08,206:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-05-05 00:29:08,206:INFO:create_model() successfully completed......................................
2025-05-05 00:29:08,471:INFO:SubProcess create_model() end ==================================
2025-05-05 00:29:08,474:INFO:Creating metrics dataframe
2025-05-05 00:29:08,532:INFO:Initializing Decision Tree Classifier
2025-05-05 00:29:08,532:INFO:Total runtime is 1.0780290722846984 minutes
2025-05-05 00:29:08,564:INFO:SubProcess create_model() called ==================================
2025-05-05 00:29:08,566:INFO:Initializing create_model()
2025-05-05 00:29:08,568:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:29:08,568:INFO:Checking exceptions
2025-05-05 00:29:08,569:INFO:Importing libraries
2025-05-05 00:29:08,570:INFO:Copying training dataset
2025-05-05 00:29:08,723:INFO:Defining folds
2025-05-05 00:29:08,723:INFO:Declaring metric variables
2025-05-05 00:29:08,743:INFO:Importing untrained model
2025-05-05 00:29:08,876:INFO:Decision Tree Classifier Imported successfully
2025-05-05 00:29:08,923:INFO:Starting cross validation
2025-05-05 00:29:08,930:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:29:20,887:INFO:Calculating mean and std
2025-05-05 00:29:20,894:INFO:Creating metrics dataframe
2025-05-05 00:29:20,928:INFO:Uploading results into container
2025-05-05 00:29:20,931:INFO:Uploading model into container now
2025-05-05 00:29:20,931:INFO:_master_model_container: 4
2025-05-05 00:29:20,934:INFO:_display_container: 2
2025-05-05 00:29:20,936:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6000, splitter='best')
2025-05-05 00:29:20,939:INFO:create_model() successfully completed......................................
2025-05-05 00:29:21,091:INFO:SubProcess create_model() end ==================================
2025-05-05 00:29:21,091:INFO:Creating metrics dataframe
2025-05-05 00:29:21,123:INFO:Initializing SVM - Linear Kernel
2025-05-05 00:29:21,124:INFO:Total runtime is 1.28789351383845 minutes
2025-05-05 00:29:21,149:INFO:SubProcess create_model() called ==================================
2025-05-05 00:29:21,151:INFO:Initializing create_model()
2025-05-05 00:29:21,156:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:29:21,156:INFO:Checking exceptions
2025-05-05 00:29:21,156:INFO:Importing libraries
2025-05-05 00:29:21,156:INFO:Copying training dataset
2025-05-05 00:29:21,278:INFO:Defining folds
2025-05-05 00:29:21,279:INFO:Declaring metric variables
2025-05-05 00:29:21,290:INFO:Importing untrained model
2025-05-05 00:29:21,314:INFO:SVM - Linear Kernel Imported successfully
2025-05-05 00:29:21,373:INFO:Starting cross validation
2025-05-05 00:29:21,383:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:29:25,137:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:25,157:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:25,173:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:25,180:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:25,248:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-05 00:29:27,747:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:27,770:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-05 00:29:28,037:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:28,069:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-05 00:29:28,087:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:28,104:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-05 00:29:28,172:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:28,196:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-05 00:29:29,491:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:29,843:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:29,939:INFO:Calculating mean and std
2025-05-05 00:29:29,945:INFO:Creating metrics dataframe
2025-05-05 00:29:29,978:INFO:Uploading results into container
2025-05-05 00:29:29,978:INFO:Uploading model into container now
2025-05-05 00:29:29,995:INFO:_master_model_container: 5
2025-05-05 00:29:29,995:INFO:_display_container: 2
2025-05-05 00:29:30,008:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6000, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-05-05 00:29:30,008:INFO:create_model() successfully completed......................................
2025-05-05 00:29:30,276:INFO:SubProcess create_model() end ==================================
2025-05-05 00:29:30,276:INFO:Creating metrics dataframe
2025-05-05 00:29:30,356:INFO:Initializing Ridge Classifier
2025-05-05 00:29:30,357:INFO:Total runtime is 1.4417737960815429 minutes
2025-05-05 00:29:30,385:INFO:SubProcess create_model() called ==================================
2025-05-05 00:29:30,385:INFO:Initializing create_model()
2025-05-05 00:29:30,392:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:29:30,393:INFO:Checking exceptions
2025-05-05 00:29:30,394:INFO:Importing libraries
2025-05-05 00:29:30,394:INFO:Copying training dataset
2025-05-05 00:29:30,546:INFO:Defining folds
2025-05-05 00:29:30,547:INFO:Declaring metric variables
2025-05-05 00:29:30,578:INFO:Importing untrained model
2025-05-05 00:29:30,590:INFO:Ridge Classifier Imported successfully
2025-05-05 00:29:30,627:INFO:Starting cross validation
2025-05-05 00:29:30,634:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:29:32,309:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:32,323:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:32,341:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:32,355:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:33,446:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:33,446:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:33,462:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:33,478:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:34,206:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:34,206:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:29:34,254:INFO:Calculating mean and std
2025-05-05 00:29:34,257:INFO:Creating metrics dataframe
2025-05-05 00:29:34,273:INFO:Uploading results into container
2025-05-05 00:29:34,273:INFO:Uploading model into container now
2025-05-05 00:29:34,278:INFO:_master_model_container: 6
2025-05-05 00:29:34,278:INFO:_display_container: 2
2025-05-05 00:29:34,278:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6000, solver='auto',
                tol=0.0001)
2025-05-05 00:29:34,278:INFO:create_model() successfully completed......................................
2025-05-05 00:29:34,406:INFO:SubProcess create_model() end ==================================
2025-05-05 00:29:34,406:INFO:Creating metrics dataframe
2025-05-05 00:29:34,445:INFO:Initializing Random Forest Classifier
2025-05-05 00:29:34,445:INFO:Total runtime is 1.5099130749702452 minutes
2025-05-05 00:29:34,463:INFO:SubProcess create_model() called ==================================
2025-05-05 00:29:34,464:INFO:Initializing create_model()
2025-05-05 00:29:34,464:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:29:34,465:INFO:Checking exceptions
2025-05-05 00:29:34,465:INFO:Importing libraries
2025-05-05 00:29:34,465:INFO:Copying training dataset
2025-05-05 00:29:34,583:INFO:Defining folds
2025-05-05 00:29:34,586:INFO:Declaring metric variables
2025-05-05 00:29:34,642:INFO:Importing untrained model
2025-05-05 00:29:34,722:INFO:Random Forest Classifier Imported successfully
2025-05-05 00:29:34,816:INFO:Starting cross validation
2025-05-05 00:29:34,823:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:32:43,466:INFO:Calculating mean and std
2025-05-05 00:32:43,466:INFO:Creating metrics dataframe
2025-05-05 00:32:43,480:INFO:Uploading results into container
2025-05-05 00:32:43,480:INFO:Uploading model into container now
2025-05-05 00:32:43,480:INFO:_master_model_container: 7
2025-05-05 00:32:43,480:INFO:_display_container: 2
2025-05-05 00:32:43,480:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6000, verbose=0,
                       warm_start=False)
2025-05-05 00:32:43,480:INFO:create_model() successfully completed......................................
2025-05-05 00:32:43,615:INFO:SubProcess create_model() end ==================================
2025-05-05 00:32:43,615:INFO:Creating metrics dataframe
2025-05-05 00:32:43,630:INFO:Initializing Quadratic Discriminant Analysis
2025-05-05 00:32:43,630:INFO:Total runtime is 4.6629985809326175 minutes
2025-05-05 00:32:43,646:INFO:SubProcess create_model() called ==================================
2025-05-05 00:32:43,646:INFO:Initializing create_model()
2025-05-05 00:32:43,646:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:32:43,646:INFO:Checking exceptions
2025-05-05 00:32:43,646:INFO:Importing libraries
2025-05-05 00:32:43,646:INFO:Copying training dataset
2025-05-05 00:32:43,830:INFO:Defining folds
2025-05-05 00:32:43,831:INFO:Declaring metric variables
2025-05-05 00:32:43,847:INFO:Importing untrained model
2025-05-05 00:32:43,865:INFO:Quadratic Discriminant Analysis Imported successfully
2025-05-05 00:32:43,895:INFO:Starting cross validation
2025-05-05 00:32:43,898:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:32:45,334:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:45,349:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:45,349:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:45,367:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:46,217:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:46,232:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:46,279:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:46,295:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:46,851:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:46,851:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:32:46,882:INFO:Calculating mean and std
2025-05-05 00:32:46,882:INFO:Creating metrics dataframe
2025-05-05 00:32:46,882:INFO:Uploading results into container
2025-05-05 00:32:46,882:INFO:Uploading model into container now
2025-05-05 00:32:46,882:INFO:_master_model_container: 8
2025-05-05 00:32:46,882:INFO:_display_container: 2
2025-05-05 00:32:46,882:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-05-05 00:32:46,882:INFO:create_model() successfully completed......................................
2025-05-05 00:32:46,981:INFO:SubProcess create_model() end ==================================
2025-05-05 00:32:46,981:INFO:Creating metrics dataframe
2025-05-05 00:32:46,997:INFO:Initializing Ada Boost Classifier
2025-05-05 00:32:46,997:INFO:Total runtime is 4.719104170799255 minutes
2025-05-05 00:32:47,012:INFO:SubProcess create_model() called ==================================
2025-05-05 00:32:47,012:INFO:Initializing create_model()
2025-05-05 00:32:47,012:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:32:47,012:INFO:Checking exceptions
2025-05-05 00:32:47,012:INFO:Importing libraries
2025-05-05 00:32:47,012:INFO:Copying training dataset
2025-05-05 00:32:47,066:INFO:Defining folds
2025-05-05 00:32:47,066:INFO:Declaring metric variables
2025-05-05 00:32:47,081:INFO:Importing untrained model
2025-05-05 00:32:47,099:INFO:Ada Boost Classifier Imported successfully
2025-05-05 00:32:47,130:INFO:Starting cross validation
2025-05-05 00:32:47,134:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:32:48,148:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:32:48,148:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:32:48,148:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:32:48,148:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:33:00,028:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:00,028:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:00,113:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:00,229:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:00,700:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:33:00,731:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:33:00,800:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:33:00,878:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:33:12,493:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:12,595:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:12,632:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:12,679:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:13,165:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:33:13,196:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 00:33:23,700:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:23,731:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:33:23,762:INFO:Calculating mean and std
2025-05-05 00:33:23,762:INFO:Creating metrics dataframe
2025-05-05 00:33:23,772:INFO:Uploading results into container
2025-05-05 00:33:23,772:INFO:Uploading model into container now
2025-05-05 00:33:23,772:INFO:_master_model_container: 9
2025-05-05 00:33:23,772:INFO:_display_container: 2
2025-05-05 00:33:23,772:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6000)
2025-05-05 00:33:23,772:INFO:create_model() successfully completed......................................
2025-05-05 00:33:23,847:INFO:SubProcess create_model() end ==================================
2025-05-05 00:33:23,847:INFO:Creating metrics dataframe
2025-05-05 00:33:23,863:INFO:Initializing Gradient Boosting Classifier
2025-05-05 00:33:23,863:INFO:Total runtime is 5.3335379481315615 minutes
2025-05-05 00:33:23,878:INFO:SubProcess create_model() called ==================================
2025-05-05 00:33:23,878:INFO:Initializing create_model()
2025-05-05 00:33:23,878:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002496DBCBEE0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024970AC9150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 00:33:23,878:INFO:Checking exceptions
2025-05-05 00:33:23,878:INFO:Importing libraries
2025-05-05 00:33:23,878:INFO:Copying training dataset
2025-05-05 00:33:23,947:INFO:Defining folds
2025-05-05 00:33:23,963:INFO:Declaring metric variables
2025-05-05 00:33:23,994:INFO:Importing untrained model
2025-05-05 00:33:24,013:INFO:Gradient Boosting Classifier Imported successfully
2025-05-05 00:33:24,063:INFO:Starting cross validation
2025-05-05 00:33:24,074:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 00:37:34,434:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:37:35,000:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:37:35,886:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:37:37,069:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:41:36,809:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:41:37,089:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:41:39,940:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 00:41:40,475:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 07:43:34,501:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 07:43:34,521:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 07:43:34,522:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 07:43:34,522:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 07:43:49,980:INFO:PyCaret ClassificationExperiment
2025-05-05 07:43:49,980:INFO:Logging name: clf-default-name
2025-05-05 07:43:49,981:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-05 07:43:49,981:INFO:version 3.3.2
2025-05-05 07:43:49,981:INFO:Initializing setup()
2025-05-05 07:43:49,982:INFO:self.USI: 94f0
2025-05-05 07:43:49,982:INFO:self._variable_keys: {'_available_plots', 'is_multiclass', 'y_test', 'html_param', 'X_train', '_ml_usecase', 'idx', 'n_jobs_param', 'y', 'X_test', 'exp_name_log', 'target_param', 'logging_param', 'fold_shuffle_param', 'seed', 'memory', 'USI', 'gpu_param', 'gpu_n_jobs_param', 'exp_id', 'log_plots_param', 'fold_generator', 'X', 'fold_groups_param', 'fix_imbalance', 'data', 'pipeline', 'y_train'}
2025-05-05 07:43:49,982:INFO:Checking environment
2025-05-05 07:43:49,983:INFO:python_version: 3.10.11
2025-05-05 07:43:49,983:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-05 07:43:49,983:INFO:machine: AMD64
2025-05-05 07:43:49,983:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-05 07:43:49,996:INFO:Memory: svmem(total=8482373632, available=1898397696, percent=77.6, used=6583975936, free=1898397696)
2025-05-05 07:43:49,996:INFO:Physical Core: 2
2025-05-05 07:43:49,996:INFO:Logical Core: 4
2025-05-05 07:43:49,996:INFO:Checking libraries
2025-05-05 07:43:49,996:INFO:System:
2025-05-05 07:43:49,996:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-05 07:43:49,996:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-05 07:43:49,996:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-05 07:43:49,996:INFO:PyCaret required dependencies:
2025-05-05 07:43:50,995:INFO:                 pip: 25.0.1
2025-05-05 07:43:50,995:INFO:          setuptools: 65.5.0
2025-05-05 07:43:50,995:INFO:             pycaret: 3.3.2
2025-05-05 07:43:50,995:INFO:             IPython: 8.34.0
2025-05-05 07:43:50,995:INFO:          ipywidgets: 8.1.5
2025-05-05 07:43:50,995:INFO:                tqdm: 4.67.1
2025-05-05 07:43:50,995:INFO:               numpy: 1.26.4
2025-05-05 07:43:50,995:INFO:              pandas: 2.1.4
2025-05-05 07:43:50,995:INFO:              jinja2: 3.1.6
2025-05-05 07:43:50,995:INFO:               scipy: 1.11.4
2025-05-05 07:43:51,010:INFO:              joblib: 1.3.2
2025-05-05 07:43:51,010:INFO:             sklearn: 1.4.2
2025-05-05 07:43:51,011:INFO:                pyod: 2.0.4
2025-05-05 07:43:51,011:INFO:            imblearn: 0.13.0
2025-05-05 07:43:51,011:INFO:   category_encoders: 2.7.0
2025-05-05 07:43:51,011:INFO:            lightgbm: 4.6.0
2025-05-05 07:43:51,011:INFO:               numba: 0.61.0
2025-05-05 07:43:51,011:INFO:            requests: 2.32.3
2025-05-05 07:43:51,011:INFO:          matplotlib: 3.7.5
2025-05-05 07:43:51,011:INFO:          scikitplot: 0.3.7
2025-05-05 07:43:51,011:INFO:         yellowbrick: 1.5
2025-05-05 07:43:51,011:INFO:              plotly: 5.24.1
2025-05-05 07:43:51,011:INFO:    plotly-resampler: Not installed
2025-05-05 07:43:51,011:INFO:             kaleido: 0.2.1
2025-05-05 07:43:51,011:INFO:           schemdraw: 0.15
2025-05-05 07:43:51,011:INFO:         statsmodels: 0.14.4
2025-05-05 07:43:51,011:INFO:              sktime: 0.26.0
2025-05-05 07:43:51,011:INFO:               tbats: 1.1.3
2025-05-05 07:43:51,011:INFO:            pmdarima: 2.0.4
2025-05-05 07:43:51,011:INFO:              psutil: 7.0.0
2025-05-05 07:43:51,011:INFO:          markupsafe: 3.0.2
2025-05-05 07:43:51,011:INFO:             pickle5: Not installed
2025-05-05 07:43:51,011:INFO:         cloudpickle: 3.1.1
2025-05-05 07:43:51,011:INFO:         deprecation: 2.1.0
2025-05-05 07:43:51,011:INFO:              xxhash: 3.5.0
2025-05-05 07:43:51,011:INFO:           wurlitzer: Not installed
2025-05-05 07:43:51,011:INFO:PyCaret optional dependencies:
2025-05-05 07:43:51,111:INFO:                shap: 0.44.1
2025-05-05 07:43:51,111:INFO:           interpret: 0.6.10
2025-05-05 07:43:51,111:INFO:                umap: 0.5.7
2025-05-05 07:43:51,111:INFO:     ydata_profiling: 4.16.1
2025-05-05 07:43:51,111:INFO:  explainerdashboard: 0.4.8
2025-05-05 07:43:51,111:INFO:             autoviz: Not installed
2025-05-05 07:43:51,111:INFO:           fairlearn: 0.7.0
2025-05-05 07:43:51,111:INFO:          deepchecks: Not installed
2025-05-05 07:43:51,111:INFO:             xgboost: Not installed
2025-05-05 07:43:51,111:INFO:            catboost: Not installed
2025-05-05 07:43:51,111:INFO:              kmodes: Not installed
2025-05-05 07:43:51,111:INFO:             mlxtend: Not installed
2025-05-05 07:43:51,111:INFO:       statsforecast: Not installed
2025-05-05 07:43:51,111:INFO:        tune_sklearn: Not installed
2025-05-05 07:43:51,111:INFO:                 ray: Not installed
2025-05-05 07:43:51,111:INFO:            hyperopt: Not installed
2025-05-05 07:43:51,111:INFO:              optuna: 4.2.1
2025-05-05 07:43:51,111:INFO:               skopt: Not installed
2025-05-05 07:43:51,111:INFO:              mlflow: Not installed
2025-05-05 07:43:51,111:INFO:              gradio: Not installed
2025-05-05 07:43:51,111:INFO:             fastapi: Not installed
2025-05-05 07:43:51,111:INFO:             uvicorn: Not installed
2025-05-05 07:43:51,111:INFO:              m2cgen: Not installed
2025-05-05 07:43:51,111:INFO:           evidently: Not installed
2025-05-05 07:43:51,111:INFO:               fugue: Not installed
2025-05-05 07:43:51,111:INFO:           streamlit: Not installed
2025-05-05 07:43:51,111:INFO:             prophet: Not installed
2025-05-05 07:43:51,111:INFO:None
2025-05-05 07:43:51,111:INFO:Set up data.
2025-05-05 07:43:51,261:INFO:Set up folding strategy.
2025-05-05 07:43:51,261:INFO:Set up train/test split.
2025-05-05 07:43:51,828:INFO:Set up index.
2025-05-05 07:43:51,871:INFO:Assigning column types.
2025-05-05 07:43:51,926:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-05 07:43:52,008:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 07:43:52,030:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:43:52,264:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:52,277:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:52,427:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 07:43:52,427:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:43:52,476:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:52,476:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:52,476:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-05 07:43:52,559:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:43:52,643:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:52,643:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:52,759:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:43:52,831:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:52,834:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:52,834:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-05 07:43:53,025:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:53,025:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:53,160:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:53,174:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:43:53,176:INFO:Preparing preprocessing pipeline...
2025-05-05 07:43:53,210:INFO:Set up label encoding.
2025-05-05 07:43:53,210:INFO:Set up simple imputation.
2025-05-05 07:43:53,261:INFO:Set up encoding of categorical features.
2025-05-05 07:43:53,261:INFO:Set up imbalanced handling.
2025-05-05 07:43:53,261:INFO:Set up feature normalization.
2025-05-05 07:43:57,028:INFO:Finished creating preprocessing pipeline.
2025-05-05 07:43:57,058:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'depth', 'table', 'price',
                                             'x', 'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-05 07:43:57,059:INFO:Creating final display dataframe.
2025-05-05 07:43:59,777:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16                Fix imbalance   
17         Fix imbalance method   
18                    Normalize   
19             Normalize method   
20               Fold Generator   
21                  Fold Number   
22                     CPU Jobs   
23                      Use GPU   
24               Log Experiment   
25              Experiment Name   
26                          USI   

                                                Value  
0                                                8154  
1                                                 cut  
2                                          Multiclass  
3   Fair: 0, Good: 1, Ideal: 2, Premium: 3, Very G...  
4                                         (53940, 10)  
5                                         (96993, 23)  
6                                         (86205, 23)  
7                                         (10788, 23)  
8                                                   7  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                               True  
17                                              smote  
18                                               True  
19                                             minmax  
20                                    StratifiedKFold  
21                                                 10  
22                                                 -1  
23                                              False  
24                                              False  
25                                   clf-default-name  
26                                               94f0  
2025-05-05 07:44:00,261:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:44:00,261:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:44:00,476:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:44:00,476:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:44:00,476:INFO:setup() successfully completed in 10.93s...............
2025-05-05 07:51:15,496:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_324\125715129.py:1: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.histplot(x="color", y="price", data=data, palette='vididis')

2025-05-05 07:51:16,748:INFO:PyCaret ClassificationExperiment
2025-05-05 07:51:16,763:INFO:Logging name: clf-default-name
2025-05-05 07:51:16,763:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-05 07:51:16,763:INFO:version 3.3.2
2025-05-05 07:51:16,763:INFO:Initializing setup()
2025-05-05 07:51:16,763:INFO:self.USI: dbbb
2025-05-05 07:51:16,763:INFO:self._variable_keys: {'_available_plots', 'is_multiclass', 'y_test', 'html_param', 'X_train', '_ml_usecase', 'idx', 'n_jobs_param', 'y', 'X_test', 'exp_name_log', 'target_param', 'logging_param', 'fold_shuffle_param', 'seed', 'memory', 'USI', 'gpu_param', 'gpu_n_jobs_param', 'exp_id', 'log_plots_param', 'fold_generator', 'X', 'fold_groups_param', 'fix_imbalance', 'data', 'pipeline', 'y_train'}
2025-05-05 07:51:16,763:INFO:Checking environment
2025-05-05 07:51:16,763:INFO:python_version: 3.10.11
2025-05-05 07:51:16,763:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-05 07:51:16,763:INFO:machine: AMD64
2025-05-05 07:51:16,763:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-05 07:51:16,763:INFO:Memory: svmem(total=8482373632, available=1574821888, percent=81.4, used=6907551744, free=1574821888)
2025-05-05 07:51:16,763:INFO:Physical Core: 2
2025-05-05 07:51:16,763:INFO:Logical Core: 4
2025-05-05 07:51:16,763:INFO:Checking libraries
2025-05-05 07:51:16,779:INFO:System:
2025-05-05 07:51:16,779:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-05 07:51:16,779:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-05 07:51:16,779:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-05 07:51:16,779:INFO:PyCaret required dependencies:
2025-05-05 07:51:16,779:INFO:                 pip: 25.0.1
2025-05-05 07:51:16,779:INFO:          setuptools: 65.5.0
2025-05-05 07:51:16,779:INFO:             pycaret: 3.3.2
2025-05-05 07:51:16,779:INFO:             IPython: 8.34.0
2025-05-05 07:51:16,779:INFO:          ipywidgets: 8.1.5
2025-05-05 07:51:16,779:INFO:                tqdm: 4.67.1
2025-05-05 07:51:16,779:INFO:               numpy: 1.26.4
2025-05-05 07:51:16,779:INFO:              pandas: 2.1.4
2025-05-05 07:51:16,779:INFO:              jinja2: 3.1.6
2025-05-05 07:51:16,779:INFO:               scipy: 1.11.4
2025-05-05 07:51:16,779:INFO:              joblib: 1.3.2
2025-05-05 07:51:16,779:INFO:             sklearn: 1.4.2
2025-05-05 07:51:16,779:INFO:                pyod: 2.0.4
2025-05-05 07:51:16,779:INFO:            imblearn: 0.13.0
2025-05-05 07:51:16,779:INFO:   category_encoders: 2.7.0
2025-05-05 07:51:16,779:INFO:            lightgbm: 4.6.0
2025-05-05 07:51:16,779:INFO:               numba: 0.61.0
2025-05-05 07:51:16,779:INFO:            requests: 2.32.3
2025-05-05 07:51:16,779:INFO:          matplotlib: 3.7.5
2025-05-05 07:51:16,779:INFO:          scikitplot: 0.3.7
2025-05-05 07:51:16,779:INFO:         yellowbrick: 1.5
2025-05-05 07:51:16,779:INFO:              plotly: 5.24.1
2025-05-05 07:51:16,779:INFO:    plotly-resampler: Not installed
2025-05-05 07:51:16,779:INFO:             kaleido: 0.2.1
2025-05-05 07:51:16,779:INFO:           schemdraw: 0.15
2025-05-05 07:51:16,779:INFO:         statsmodels: 0.14.4
2025-05-05 07:51:16,779:INFO:              sktime: 0.26.0
2025-05-05 07:51:16,779:INFO:               tbats: 1.1.3
2025-05-05 07:51:16,779:INFO:            pmdarima: 2.0.4
2025-05-05 07:51:16,779:INFO:              psutil: 7.0.0
2025-05-05 07:51:16,779:INFO:          markupsafe: 3.0.2
2025-05-05 07:51:16,794:INFO:             pickle5: Not installed
2025-05-05 07:51:16,794:INFO:         cloudpickle: 3.1.1
2025-05-05 07:51:16,794:INFO:         deprecation: 2.1.0
2025-05-05 07:51:16,794:INFO:              xxhash: 3.5.0
2025-05-05 07:51:16,794:INFO:           wurlitzer: Not installed
2025-05-05 07:51:16,794:INFO:PyCaret optional dependencies:
2025-05-05 07:51:16,794:INFO:                shap: 0.44.1
2025-05-05 07:51:16,794:INFO:           interpret: 0.6.10
2025-05-05 07:51:16,794:INFO:                umap: 0.5.7
2025-05-05 07:51:16,794:INFO:     ydata_profiling: 4.16.1
2025-05-05 07:51:16,794:INFO:  explainerdashboard: 0.4.8
2025-05-05 07:51:16,794:INFO:             autoviz: Not installed
2025-05-05 07:51:16,794:INFO:           fairlearn: 0.7.0
2025-05-05 07:51:16,794:INFO:          deepchecks: Not installed
2025-05-05 07:51:16,794:INFO:             xgboost: Not installed
2025-05-05 07:51:16,794:INFO:            catboost: Not installed
2025-05-05 07:51:16,794:INFO:              kmodes: Not installed
2025-05-05 07:51:16,794:INFO:             mlxtend: Not installed
2025-05-05 07:51:16,794:INFO:       statsforecast: Not installed
2025-05-05 07:51:16,794:INFO:        tune_sklearn: Not installed
2025-05-05 07:51:16,794:INFO:                 ray: Not installed
2025-05-05 07:51:16,794:INFO:            hyperopt: Not installed
2025-05-05 07:51:16,794:INFO:              optuna: 4.2.1
2025-05-05 07:51:16,794:INFO:               skopt: Not installed
2025-05-05 07:51:16,794:INFO:              mlflow: Not installed
2025-05-05 07:51:16,794:INFO:              gradio: Not installed
2025-05-05 07:51:16,794:INFO:             fastapi: Not installed
2025-05-05 07:51:16,794:INFO:             uvicorn: Not installed
2025-05-05 07:51:16,794:INFO:              m2cgen: Not installed
2025-05-05 07:51:16,794:INFO:           evidently: Not installed
2025-05-05 07:51:16,794:INFO:               fugue: Not installed
2025-05-05 07:51:16,794:INFO:           streamlit: Not installed
2025-05-05 07:51:16,794:INFO:             prophet: Not installed
2025-05-05 07:51:16,794:INFO:None
2025-05-05 07:51:16,794:INFO:Set up data.
2025-05-05 07:51:16,888:INFO:Set up folding strategy.
2025-05-05 07:51:16,888:INFO:Set up train/test split.
2025-05-05 07:51:17,095:INFO:Set up index.
2025-05-05 07:51:17,097:INFO:Assigning column types.
2025-05-05 07:51:17,120:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-05 07:51:17,205:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 07:51:17,205:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:51:17,283:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:17,283:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:17,483:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 07:51:17,488:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:51:17,572:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:17,572:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:17,578:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-05 07:51:17,845:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:51:17,937:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:17,938:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:18,192:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:51:18,263:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:18,263:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:18,263:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-05 07:51:18,452:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:18,453:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:18,658:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:18,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:18,666:INFO:Preparing preprocessing pipeline...
2025-05-05 07:51:18,671:INFO:Set up label encoding.
2025-05-05 07:51:18,671:INFO:Set up simple imputation.
2025-05-05 07:51:18,689:INFO:Set up encoding of categorical features.
2025-05-05 07:51:18,689:INFO:Set up imbalanced handling.
2025-05-05 07:51:18,689:INFO:Set up feature normalization.
2025-05-05 07:51:21,570:INFO:Finished creating preprocessing pipeline.
2025-05-05 07:51:21,589:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'depth', 'table', 'price',
                                             'x', 'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-05 07:51:21,589:INFO:Creating final display dataframe.
2025-05-05 07:51:25,188:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16                Fix imbalance   
17         Fix imbalance method   
18                    Normalize   
19             Normalize method   
20               Fold Generator   
21                  Fold Number   
22                     CPU Jobs   
23                      Use GPU   
24               Log Experiment   
25              Experiment Name   
26                          USI   

                                                Value  
0                                                8075  
1                                                 cut  
2                                          Multiclass  
3   Fair: 0, Good: 1, Ideal: 2, Premium: 3, Very G...  
4                                         (53940, 10)  
5                                         (96993, 23)  
6                                         (86205, 23)  
7                                         (10788, 23)  
8                                                   7  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                               True  
17                                              smote  
18                                               True  
19                                             minmax  
20                                    StratifiedKFold  
21                                                 10  
22                                                 -1  
23                                              False  
24                                              False  
25                                   clf-default-name  
26                                               dbbb  
2025-05-05 07:51:25,498:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:25,498:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:25,666:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:25,667:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:51:25,672:INFO:setup() successfully completed in 9.26s...............
2025-05-05 07:51:37,383:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_324\81556738.py:1: UserWarning: Ignoring `palette` because no `hue` variable has been assigned.
  sns.histplot(x="color", y="price", data=data, palette='muted')

2025-05-05 07:53:38,653:INFO:PyCaret ClassificationExperiment
2025-05-05 07:53:38,653:INFO:Logging name: clf-default-name
2025-05-05 07:53:38,653:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-05 07:53:38,653:INFO:version 3.3.2
2025-05-05 07:53:38,653:INFO:Initializing setup()
2025-05-05 07:53:38,654:INFO:self.USI: e811
2025-05-05 07:53:38,654:INFO:self._variable_keys: {'_available_plots', 'is_multiclass', 'y_test', 'html_param', 'X_train', '_ml_usecase', 'idx', 'n_jobs_param', 'y', 'X_test', 'exp_name_log', 'target_param', 'logging_param', 'fold_shuffle_param', 'seed', 'memory', 'USI', 'gpu_param', 'gpu_n_jobs_param', 'exp_id', 'log_plots_param', 'fold_generator', 'X', 'fold_groups_param', 'fix_imbalance', 'data', 'pipeline', 'y_train'}
2025-05-05 07:53:38,654:INFO:Checking environment
2025-05-05 07:53:38,654:INFO:python_version: 3.10.11
2025-05-05 07:53:38,654:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-05 07:53:38,654:INFO:machine: AMD64
2025-05-05 07:53:38,654:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-05 07:53:38,654:INFO:Memory: svmem(total=8482373632, available=1746321408, percent=79.4, used=6736052224, free=1746321408)
2025-05-05 07:53:38,654:INFO:Physical Core: 2
2025-05-05 07:53:38,654:INFO:Logical Core: 4
2025-05-05 07:53:38,654:INFO:Checking libraries
2025-05-05 07:53:38,654:INFO:System:
2025-05-05 07:53:38,654:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-05 07:53:38,654:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-05 07:53:38,654:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-05 07:53:38,654:INFO:PyCaret required dependencies:
2025-05-05 07:53:38,654:INFO:                 pip: 25.0.1
2025-05-05 07:53:38,654:INFO:          setuptools: 65.5.0
2025-05-05 07:53:38,654:INFO:             pycaret: 3.3.2
2025-05-05 07:53:38,654:INFO:             IPython: 8.34.0
2025-05-05 07:53:38,654:INFO:          ipywidgets: 8.1.5
2025-05-05 07:53:38,654:INFO:                tqdm: 4.67.1
2025-05-05 07:53:38,654:INFO:               numpy: 1.26.4
2025-05-05 07:53:38,654:INFO:              pandas: 2.1.4
2025-05-05 07:53:38,654:INFO:              jinja2: 3.1.6
2025-05-05 07:53:38,654:INFO:               scipy: 1.11.4
2025-05-05 07:53:38,654:INFO:              joblib: 1.3.2
2025-05-05 07:53:38,654:INFO:             sklearn: 1.4.2
2025-05-05 07:53:38,654:INFO:                pyod: 2.0.4
2025-05-05 07:53:38,654:INFO:            imblearn: 0.13.0
2025-05-05 07:53:38,654:INFO:   category_encoders: 2.7.0
2025-05-05 07:53:38,654:INFO:            lightgbm: 4.6.0
2025-05-05 07:53:38,654:INFO:               numba: 0.61.0
2025-05-05 07:53:38,654:INFO:            requests: 2.32.3
2025-05-05 07:53:38,654:INFO:          matplotlib: 3.7.5
2025-05-05 07:53:38,654:INFO:          scikitplot: 0.3.7
2025-05-05 07:53:38,654:INFO:         yellowbrick: 1.5
2025-05-05 07:53:38,654:INFO:              plotly: 5.24.1
2025-05-05 07:53:38,654:INFO:    plotly-resampler: Not installed
2025-05-05 07:53:38,654:INFO:             kaleido: 0.2.1
2025-05-05 07:53:38,654:INFO:           schemdraw: 0.15
2025-05-05 07:53:38,654:INFO:         statsmodels: 0.14.4
2025-05-05 07:53:38,654:INFO:              sktime: 0.26.0
2025-05-05 07:53:38,654:INFO:               tbats: 1.1.3
2025-05-05 07:53:38,654:INFO:            pmdarima: 2.0.4
2025-05-05 07:53:38,654:INFO:              psutil: 7.0.0
2025-05-05 07:53:38,654:INFO:          markupsafe: 3.0.2
2025-05-05 07:53:38,664:INFO:             pickle5: Not installed
2025-05-05 07:53:38,664:INFO:         cloudpickle: 3.1.1
2025-05-05 07:53:38,664:INFO:         deprecation: 2.1.0
2025-05-05 07:53:38,664:INFO:              xxhash: 3.5.0
2025-05-05 07:53:38,665:INFO:           wurlitzer: Not installed
2025-05-05 07:53:38,665:INFO:PyCaret optional dependencies:
2025-05-05 07:53:38,665:INFO:                shap: 0.44.1
2025-05-05 07:53:38,665:INFO:           interpret: 0.6.10
2025-05-05 07:53:38,665:INFO:                umap: 0.5.7
2025-05-05 07:53:38,665:INFO:     ydata_profiling: 4.16.1
2025-05-05 07:53:38,665:INFO:  explainerdashboard: 0.4.8
2025-05-05 07:53:38,665:INFO:             autoviz: Not installed
2025-05-05 07:53:38,665:INFO:           fairlearn: 0.7.0
2025-05-05 07:53:38,665:INFO:          deepchecks: Not installed
2025-05-05 07:53:38,665:INFO:             xgboost: Not installed
2025-05-05 07:53:38,665:INFO:            catboost: Not installed
2025-05-05 07:53:38,665:INFO:              kmodes: Not installed
2025-05-05 07:53:38,665:INFO:             mlxtend: Not installed
2025-05-05 07:53:38,665:INFO:       statsforecast: Not installed
2025-05-05 07:53:38,665:INFO:        tune_sklearn: Not installed
2025-05-05 07:53:38,665:INFO:                 ray: Not installed
2025-05-05 07:53:38,665:INFO:            hyperopt: Not installed
2025-05-05 07:53:38,665:INFO:              optuna: 4.2.1
2025-05-05 07:53:38,665:INFO:               skopt: Not installed
2025-05-05 07:53:38,665:INFO:              mlflow: Not installed
2025-05-05 07:53:38,665:INFO:              gradio: Not installed
2025-05-05 07:53:38,665:INFO:             fastapi: Not installed
2025-05-05 07:53:38,665:INFO:             uvicorn: Not installed
2025-05-05 07:53:38,665:INFO:              m2cgen: Not installed
2025-05-05 07:53:38,665:INFO:           evidently: Not installed
2025-05-05 07:53:38,665:INFO:               fugue: Not installed
2025-05-05 07:53:38,665:INFO:           streamlit: Not installed
2025-05-05 07:53:38,665:INFO:             prophet: Not installed
2025-05-05 07:53:38,665:INFO:None
2025-05-05 07:53:38,665:INFO:Set up data.
2025-05-05 07:53:38,717:INFO:Set up folding strategy.
2025-05-05 07:53:38,722:INFO:Set up train/test split.
2025-05-05 07:53:38,848:INFO:Set up index.
2025-05-05 07:53:38,848:INFO:Assigning column types.
2025-05-05 07:53:38,884:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-05 07:53:39,000:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 07:53:39,000:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:53:39,065:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,065:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,165:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 07:53:39,165:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:53:39,249:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,249:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,249:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-05 07:53:39,366:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:53:39,415:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,415:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,492:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 07:53:39,548:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,548:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-05 07:53:39,882:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:39,883:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:40,049:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:40,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:40,049:INFO:Preparing preprocessing pipeline...
2025-05-05 07:53:40,067:INFO:Set up label encoding.
2025-05-05 07:53:40,067:INFO:Set up simple imputation.
2025-05-05 07:53:40,083:INFO:Set up encoding of categorical features.
2025-05-05 07:53:40,083:INFO:Set up imbalanced handling.
2025-05-05 07:53:40,083:INFO:Set up feature normalization.
2025-05-05 07:53:42,676:INFO:Finished creating preprocessing pipeline.
2025-05-05 07:53:42,754:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'depth', 'table', 'price',
                                             'x', 'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-05 07:53:42,754:INFO:Creating final display dataframe.
2025-05-05 07:53:45,916:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16                Fix imbalance   
17         Fix imbalance method   
18                    Normalize   
19             Normalize method   
20               Fold Generator   
21                  Fold Number   
22                     CPU Jobs   
23                      Use GPU   
24               Log Experiment   
25              Experiment Name   
26                          USI   

                                                Value  
0                                                6913  
1                                                 cut  
2                                          Multiclass  
3   Fair: 0, Good: 1, Ideal: 2, Premium: 3, Very G...  
4                                         (53940, 10)  
5                                         (96993, 23)  
6                                         (86205, 23)  
7                                         (10788, 23)  
8                                                   7  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                               True  
17                                              smote  
18                                               True  
19                                             minmax  
20                                    StratifiedKFold  
21                                                 10  
22                                                 -1  
23                                              False  
24                                              False  
25                                   clf-default-name  
26                                               e811  
2025-05-05 07:53:46,166:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:46,166:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:46,348:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:46,348:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 07:53:46,348:INFO:setup() successfully completed in 8.03s...............
2025-05-05 07:54:07,365:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_324\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-05 07:59:21,037:INFO:Initializing compare_models()
2025-05-05 07:59:21,037:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A869A9CC0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024A869A9CC0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-05-05 07:59:21,037:INFO:Checking exceptions
2025-05-05 07:59:21,083:INFO:Preparing display monitor
2025-05-05 07:59:21,297:INFO:Initializing Logistic Regression
2025-05-05 07:59:21,297:INFO:Total runtime is 0.0 minutes
2025-05-05 07:59:21,315:INFO:SubProcess create_model() called ==================================
2025-05-05 07:59:21,315:INFO:Initializing create_model()
2025-05-05 07:59:21,315:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A869A9CC0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024AEE8BF130>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 07:59:21,315:INFO:Checking exceptions
2025-05-05 07:59:21,315:INFO:Importing libraries
2025-05-05 07:59:21,315:INFO:Copying training dataset
2025-05-05 07:59:21,624:INFO:Defining folds
2025-05-05 07:59:21,624:INFO:Declaring metric variables
2025-05-05 07:59:21,656:INFO:Importing untrained model
2025-05-05 07:59:21,740:INFO:Logistic Regression Imported successfully
2025-05-05 07:59:21,863:INFO:Starting cross validation
2025-05-05 07:59:21,876:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 07:59:59,669:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 07:59:59,747:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 07:59:59,820:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 07:59:59,938:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,095:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:00,134:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,218:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,265:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:00,286:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,286:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,365:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,432:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,527:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:00,562:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,667:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:00,752:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:25,229:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:25,263:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:25,385:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:25,451:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:25,860:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:25,877:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:25,893:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:25,922:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:25,979:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:26,013:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:26,079:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:26,079:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:26,496:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:26,512:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:26,595:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:26,668:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:40,582:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:40,596:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:40,646:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:40,678:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:41,730:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:00:41,746:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:41,819:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:41,862:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:00:41,928:INFO:Calculating mean and std
2025-05-05 08:00:41,932:INFO:Creating metrics dataframe
2025-05-05 08:00:41,940:INFO:Uploading results into container
2025-05-05 08:00:41,941:INFO:Uploading model into container now
2025-05-05 08:00:41,946:INFO:_master_model_container: 1
2025-05-05 08:00:41,946:INFO:_display_container: 2
2025-05-05 08:00:41,948:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6913, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-05 08:00:41,949:INFO:create_model() successfully completed......................................
2025-05-05 08:00:42,214:INFO:SubProcess create_model() end ==================================
2025-05-05 08:00:42,215:INFO:Creating metrics dataframe
2025-05-05 08:00:42,235:INFO:Initializing K Neighbors Classifier
2025-05-05 08:00:42,235:INFO:Total runtime is 1.3489706635475158 minutes
2025-05-05 08:00:42,252:INFO:SubProcess create_model() called ==================================
2025-05-05 08:00:42,252:INFO:Initializing create_model()
2025-05-05 08:00:42,253:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A869A9CC0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024AEE8BF130>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:00:42,254:INFO:Checking exceptions
2025-05-05 08:00:42,254:INFO:Importing libraries
2025-05-05 08:00:42,254:INFO:Copying training dataset
2025-05-05 08:00:42,421:INFO:Defining folds
2025-05-05 08:00:42,421:INFO:Declaring metric variables
2025-05-05 08:00:42,535:INFO:Importing untrained model
2025-05-05 08:00:42,613:INFO:K Neighbors Classifier Imported successfully
2025-05-05 08:00:42,748:INFO:Starting cross validation
2025-05-05 08:00:42,762:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:01:07,278:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_324\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-05 08:01:09,555:INFO:PyCaret ClassificationExperiment
2025-05-05 08:01:09,555:INFO:Logging name: clf-default-name
2025-05-05 08:01:09,555:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-05 08:01:09,555:INFO:version 3.3.2
2025-05-05 08:01:09,555:INFO:Initializing setup()
2025-05-05 08:01:09,555:INFO:self.USI: 8458
2025-05-05 08:01:09,556:INFO:self._variable_keys: {'_available_plots', 'is_multiclass', 'y_test', 'html_param', 'X_train', '_ml_usecase', 'idx', 'n_jobs_param', 'y', 'X_test', 'exp_name_log', 'target_param', 'logging_param', 'fold_shuffle_param', 'seed', 'memory', 'USI', 'gpu_param', 'gpu_n_jobs_param', 'exp_id', 'log_plots_param', 'fold_generator', 'X', 'fold_groups_param', 'fix_imbalance', 'data', 'pipeline', 'y_train'}
2025-05-05 08:01:09,556:INFO:Checking environment
2025-05-05 08:01:09,556:INFO:python_version: 3.10.11
2025-05-05 08:01:09,556:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-05 08:01:09,556:INFO:machine: AMD64
2025-05-05 08:01:09,556:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-05 08:01:09,561:INFO:Memory: svmem(total=8482373632, available=1862156288, percent=78.0, used=6620217344, free=1862156288)
2025-05-05 08:01:09,561:INFO:Physical Core: 2
2025-05-05 08:01:09,561:INFO:Logical Core: 4
2025-05-05 08:01:09,561:INFO:Checking libraries
2025-05-05 08:01:09,561:INFO:System:
2025-05-05 08:01:09,562:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-05 08:01:09,562:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-05 08:01:09,562:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-05 08:01:09,562:INFO:PyCaret required dependencies:
2025-05-05 08:01:09,562:INFO:                 pip: 25.0.1
2025-05-05 08:01:09,562:INFO:          setuptools: 65.5.0
2025-05-05 08:01:09,562:INFO:             pycaret: 3.3.2
2025-05-05 08:01:09,562:INFO:             IPython: 8.34.0
2025-05-05 08:01:09,562:INFO:          ipywidgets: 8.1.5
2025-05-05 08:01:09,562:INFO:                tqdm: 4.67.1
2025-05-05 08:01:09,562:INFO:               numpy: 1.26.4
2025-05-05 08:01:09,562:INFO:              pandas: 2.1.4
2025-05-05 08:01:09,562:INFO:              jinja2: 3.1.6
2025-05-05 08:01:09,562:INFO:               scipy: 1.11.4
2025-05-05 08:01:09,562:INFO:              joblib: 1.3.2
2025-05-05 08:01:09,562:INFO:             sklearn: 1.4.2
2025-05-05 08:01:09,562:INFO:                pyod: 2.0.4
2025-05-05 08:01:09,562:INFO:            imblearn: 0.13.0
2025-05-05 08:01:09,562:INFO:   category_encoders: 2.7.0
2025-05-05 08:01:09,562:INFO:            lightgbm: 4.6.0
2025-05-05 08:01:09,562:INFO:               numba: 0.61.0
2025-05-05 08:01:09,562:INFO:            requests: 2.32.3
2025-05-05 08:01:09,562:INFO:          matplotlib: 3.7.5
2025-05-05 08:01:09,562:INFO:          scikitplot: 0.3.7
2025-05-05 08:01:09,562:INFO:         yellowbrick: 1.5
2025-05-05 08:01:09,562:INFO:              plotly: 5.24.1
2025-05-05 08:01:09,562:INFO:    plotly-resampler: Not installed
2025-05-05 08:01:09,562:INFO:             kaleido: 0.2.1
2025-05-05 08:01:09,562:INFO:           schemdraw: 0.15
2025-05-05 08:01:09,562:INFO:         statsmodels: 0.14.4
2025-05-05 08:01:09,562:INFO:              sktime: 0.26.0
2025-05-05 08:01:09,562:INFO:               tbats: 1.1.3
2025-05-05 08:01:09,562:INFO:            pmdarima: 2.0.4
2025-05-05 08:01:09,562:INFO:              psutil: 7.0.0
2025-05-05 08:01:09,562:INFO:          markupsafe: 3.0.2
2025-05-05 08:01:09,562:INFO:             pickle5: Not installed
2025-05-05 08:01:09,562:INFO:         cloudpickle: 3.1.1
2025-05-05 08:01:09,562:INFO:         deprecation: 2.1.0
2025-05-05 08:01:09,562:INFO:              xxhash: 3.5.0
2025-05-05 08:01:09,562:INFO:           wurlitzer: Not installed
2025-05-05 08:01:09,562:INFO:PyCaret optional dependencies:
2025-05-05 08:01:09,562:INFO:                shap: 0.44.1
2025-05-05 08:01:09,562:INFO:           interpret: 0.6.10
2025-05-05 08:01:09,562:INFO:                umap: 0.5.7
2025-05-05 08:01:09,562:INFO:     ydata_profiling: 4.16.1
2025-05-05 08:01:09,562:INFO:  explainerdashboard: 0.4.8
2025-05-05 08:01:09,562:INFO:             autoviz: Not installed
2025-05-05 08:01:09,562:INFO:           fairlearn: 0.7.0
2025-05-05 08:01:09,562:INFO:          deepchecks: Not installed
2025-05-05 08:01:09,562:INFO:             xgboost: Not installed
2025-05-05 08:01:09,562:INFO:            catboost: Not installed
2025-05-05 08:01:09,562:INFO:              kmodes: Not installed
2025-05-05 08:01:09,562:INFO:             mlxtend: Not installed
2025-05-05 08:01:09,562:INFO:       statsforecast: Not installed
2025-05-05 08:01:09,562:INFO:        tune_sklearn: Not installed
2025-05-05 08:01:09,562:INFO:                 ray: Not installed
2025-05-05 08:01:09,562:INFO:            hyperopt: Not installed
2025-05-05 08:01:09,562:INFO:              optuna: 4.2.1
2025-05-05 08:01:09,562:INFO:               skopt: Not installed
2025-05-05 08:01:09,562:INFO:              mlflow: Not installed
2025-05-05 08:01:09,562:INFO:              gradio: Not installed
2025-05-05 08:01:09,562:INFO:             fastapi: Not installed
2025-05-05 08:01:09,562:INFO:             uvicorn: Not installed
2025-05-05 08:01:09,562:INFO:              m2cgen: Not installed
2025-05-05 08:01:09,562:INFO:           evidently: Not installed
2025-05-05 08:01:09,562:INFO:               fugue: Not installed
2025-05-05 08:01:09,562:INFO:           streamlit: Not installed
2025-05-05 08:01:09,562:INFO:             prophet: Not installed
2025-05-05 08:01:09,562:INFO:None
2025-05-05 08:01:09,562:INFO:Set up data.
2025-05-05 08:01:09,614:INFO:Set up folding strategy.
2025-05-05 08:01:09,619:INFO:Set up train/test split.
2025-05-05 08:01:09,730:INFO:Set up index.
2025-05-05 08:01:09,740:INFO:Assigning column types.
2025-05-05 08:01:09,763:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-05 08:01:09,880:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 08:01:09,880:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 08:01:09,994:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:09,996:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,112:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 08:01:10,112:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 08:01:10,186:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,187:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,187:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-05 08:01:10,283:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 08:01:10,329:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,329:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,446:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 08:01:10,495:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,495:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,495:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-05 08:01:10,661:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,661:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,836:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,837:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:10,837:INFO:Preparing preprocessing pipeline...
2025-05-05 08:01:10,844:INFO:Set up label encoding.
2025-05-05 08:01:10,844:INFO:Set up simple imputation.
2025-05-05 08:01:10,861:INFO:Set up encoding of categorical features.
2025-05-05 08:01:10,861:INFO:Set up imbalanced handling.
2025-05-05 08:01:10,861:INFO:Set up feature normalization.
2025-05-05 08:01:13,710:INFO:Finished creating preprocessing pipeline.
2025-05-05 08:01:13,746:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'depth', 'table', 'price',
                                             'x', 'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-05 08:01:13,746:INFO:Creating final display dataframe.
2025-05-05 08:01:18,405:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16                Fix imbalance   
17         Fix imbalance method   
18                    Normalize   
19             Normalize method   
20               Fold Generator   
21                  Fold Number   
22                     CPU Jobs   
23                      Use GPU   
24               Log Experiment   
25              Experiment Name   
26                          USI   

                                                Value  
0                                                6757  
1                                                 cut  
2                                          Multiclass  
3   Fair: 0, Good: 1, Ideal: 2, Premium: 3, Very G...  
4                                         (53940, 10)  
5                                         (96993, 23)  
6                                         (86205, 23)  
7                                         (10788, 23)  
8                                                   7  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                               True  
17                                              smote  
18                                               True  
19                                             minmax  
20                                    StratifiedKFold  
21                                                 10  
22                                                 -1  
23                                              False  
24                                              False  
25                                   clf-default-name  
26                                               8458  
2025-05-05 08:01:18,836:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:18,836:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:19,196:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:19,196:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 08:01:19,196:INFO:setup() successfully completed in 9.88s...............
2025-05-05 08:01:19,391:INFO:Initializing compare_models()
2025-05-05 08:01:19,392:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-05-05 08:01:19,392:INFO:Checking exceptions
2025-05-05 08:01:19,442:INFO:Preparing display monitor
2025-05-05 08:01:19,581:INFO:Initializing Logistic Regression
2025-05-05 08:01:19,581:INFO:Total runtime is 0.0 minutes
2025-05-05 08:01:19,615:INFO:SubProcess create_model() called ==================================
2025-05-05 08:01:19,615:INFO:Initializing create_model()
2025-05-05 08:01:19,615:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:01:19,615:INFO:Checking exceptions
2025-05-05 08:01:19,615:INFO:Importing libraries
2025-05-05 08:01:19,615:INFO:Copying training dataset
2025-05-05 08:01:19,841:INFO:Defining folds
2025-05-05 08:01:19,841:INFO:Declaring metric variables
2025-05-05 08:01:19,967:INFO:Importing untrained model
2025-05-05 08:01:19,991:INFO:Logistic Regression Imported successfully
2025-05-05 08:01:20,112:INFO:Starting cross validation
2025-05-05 08:01:20,120:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:02:04,910:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:04,948:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:04,964:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:04,996:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:05,022:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:05,054:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:05,088:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:05,115:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:05,672:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:05,732:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:05,840:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:05,922:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:06,289:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:06,358:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:06,539:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:06,816:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:31,547:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:31,574:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:31,647:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:31,657:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:31,713:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:31,733:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:31,779:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:31,847:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:32,140:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:32,197:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:32,267:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:32,338:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:32,890:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:32,920:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:33,008:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:33,073:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:48,306:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:48,335:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:48,407:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:48,466:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:49,063:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:02:49,074:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:49,141:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:49,208:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:02:49,323:INFO:Calculating mean and std
2025-05-05 08:02:49,324:INFO:Creating metrics dataframe
2025-05-05 08:02:49,324:INFO:Uploading results into container
2025-05-05 08:02:49,324:INFO:Uploading model into container now
2025-05-05 08:02:49,324:INFO:_master_model_container: 1
2025-05-05 08:02:49,324:INFO:_display_container: 2
2025-05-05 08:02:49,340:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6757, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-05 08:02:49,340:INFO:create_model() successfully completed......................................
2025-05-05 08:02:49,624:INFO:SubProcess create_model() end ==================================
2025-05-05 08:02:49,624:INFO:Creating metrics dataframe
2025-05-05 08:02:49,667:INFO:Initializing K Neighbors Classifier
2025-05-05 08:02:49,667:INFO:Total runtime is 1.5014254371325175 minutes
2025-05-05 08:02:49,697:INFO:SubProcess create_model() called ==================================
2025-05-05 08:02:49,697:INFO:Initializing create_model()
2025-05-05 08:02:49,697:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:02:49,697:INFO:Checking exceptions
2025-05-05 08:02:49,697:INFO:Importing libraries
2025-05-05 08:02:49,701:INFO:Copying training dataset
2025-05-05 08:02:50,340:INFO:Defining folds
2025-05-05 08:02:50,340:INFO:Declaring metric variables
2025-05-05 08:02:50,360:INFO:Importing untrained model
2025-05-05 08:02:50,456:INFO:K Neighbors Classifier Imported successfully
2025-05-05 08:02:50,665:INFO:Starting cross validation
2025-05-05 08:02:50,673:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:03:08,201:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,259:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,259:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,324:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,324:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,494:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,511:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,703:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,703:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,789:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,828:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:08,920:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:23,695:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:23,719:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:23,758:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:23,788:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:23,823:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:23,837:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:23,988:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:24,011:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:24,059:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:24,075:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:24,125:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:24,140:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:41,511:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:41,511:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:41,582:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:41,582:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:41,643:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:41,643:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:41,771:INFO:Calculating mean and std
2025-05-05 08:03:41,776:INFO:Creating metrics dataframe
2025-05-05 08:03:41,786:INFO:Uploading results into container
2025-05-05 08:03:41,790:INFO:Uploading model into container now
2025-05-05 08:03:41,791:INFO:_master_model_container: 2
2025-05-05 08:03:41,791:INFO:_display_container: 2
2025-05-05 08:03:41,798:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-05-05 08:03:41,798:INFO:create_model() successfully completed......................................
2025-05-05 08:03:42,074:INFO:SubProcess create_model() end ==================================
2025-05-05 08:03:42,074:INFO:Creating metrics dataframe
2025-05-05 08:03:42,114:INFO:Initializing Naive Bayes
2025-05-05 08:03:42,114:INFO:Total runtime is 2.3755480090777077 minutes
2025-05-05 08:03:42,131:INFO:SubProcess create_model() called ==================================
2025-05-05 08:03:42,132:INFO:Initializing create_model()
2025-05-05 08:03:42,132:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:03:42,133:INFO:Checking exceptions
2025-05-05 08:03:42,133:INFO:Importing libraries
2025-05-05 08:03:42,134:INFO:Copying training dataset
2025-05-05 08:03:42,226:INFO:Defining folds
2025-05-05 08:03:42,226:INFO:Declaring metric variables
2025-05-05 08:03:42,275:INFO:Importing untrained model
2025-05-05 08:03:42,336:INFO:Naive Bayes Imported successfully
2025-05-05 08:03:42,398:INFO:Starting cross validation
2025-05-05 08:03:42,401:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:03:47,935:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:47,952:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:47,985:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,002:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,017:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,035:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,068:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,068:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,068:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,102:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,141:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:48,219:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:52,935:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:52,985:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,002:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,002:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,041:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,054:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,068:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,068:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,085:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,118:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,118:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:53,158:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:57,898:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:57,968:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:57,971:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:58,004:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:58,014:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:58,053:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:03:58,102:INFO:Calculating mean and std
2025-05-05 08:03:58,104:INFO:Creating metrics dataframe
2025-05-05 08:03:58,108:INFO:Uploading results into container
2025-05-05 08:03:58,108:INFO:Uploading model into container now
2025-05-05 08:03:58,109:INFO:_master_model_container: 3
2025-05-05 08:03:58,110:INFO:_display_container: 2
2025-05-05 08:03:58,111:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-05-05 08:03:58,113:INFO:create_model() successfully completed......................................
2025-05-05 08:03:58,369:INFO:SubProcess create_model() end ==================================
2025-05-05 08:03:58,369:INFO:Creating metrics dataframe
2025-05-05 08:03:58,436:INFO:Initializing Decision Tree Classifier
2025-05-05 08:03:58,436:INFO:Total runtime is 2.647582892576853 minutes
2025-05-05 08:03:58,467:INFO:SubProcess create_model() called ==================================
2025-05-05 08:03:58,467:INFO:Initializing create_model()
2025-05-05 08:03:58,472:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:03:58,472:INFO:Checking exceptions
2025-05-05 08:03:58,473:INFO:Importing libraries
2025-05-05 08:03:58,473:INFO:Copying training dataset
2025-05-05 08:03:58,637:INFO:Defining folds
2025-05-05 08:03:58,637:INFO:Declaring metric variables
2025-05-05 08:03:58,658:INFO:Importing untrained model
2025-05-05 08:03:58,682:INFO:Decision Tree Classifier Imported successfully
2025-05-05 08:03:58,731:INFO:Starting cross validation
2025-05-05 08:03:58,738:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:04:08,151:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,233:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,251:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,251:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,267:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,301:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,301:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,317:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,317:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,358:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,367:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:08,384:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,783:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,800:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,817:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,850:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,850:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,850:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,883:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,906:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,917:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,917:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,934:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:17,969:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:26,771:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:26,812:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:26,839:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:26,869:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:26,899:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:26,931:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:27,007:INFO:Calculating mean and std
2025-05-05 08:04:27,010:INFO:Creating metrics dataframe
2025-05-05 08:04:27,015:INFO:Uploading results into container
2025-05-05 08:04:27,017:INFO:Uploading model into container now
2025-05-05 08:04:27,019:INFO:_master_model_container: 4
2025-05-05 08:04:27,019:INFO:_display_container: 2
2025-05-05 08:04:27,021:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=6757, splitter='best')
2025-05-05 08:04:27,021:INFO:create_model() successfully completed......................................
2025-05-05 08:04:27,320:INFO:SubProcess create_model() end ==================================
2025-05-05 08:04:27,320:INFO:Creating metrics dataframe
2025-05-05 08:04:27,350:INFO:Initializing SVM - Linear Kernel
2025-05-05 08:04:27,351:INFO:Total runtime is 3.1294917066891985 minutes
2025-05-05 08:04:27,372:INFO:SubProcess create_model() called ==================================
2025-05-05 08:04:27,373:INFO:Initializing create_model()
2025-05-05 08:04:27,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:04:27,373:INFO:Checking exceptions
2025-05-05 08:04:27,373:INFO:Importing libraries
2025-05-05 08:04:27,373:INFO:Copying training dataset
2025-05-05 08:04:27,562:INFO:Defining folds
2025-05-05 08:04:27,563:INFO:Declaring metric variables
2025-05-05 08:04:27,588:INFO:Importing untrained model
2025-05-05 08:04:27,633:INFO:SVM - Linear Kernel Imported successfully
2025-05-05 08:04:27,683:INFO:Starting cross validation
2025-05-05 08:04:27,692:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:04:35,350:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:04:35,350:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:04:35,383:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,383:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,383:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:04:35,399:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:04:35,416:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,416:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,448:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,449:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,483:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,483:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,483:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-05 08:04:35,507:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,524:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,588:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:35,642:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:53,299:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:04:53,385:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:53,452:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:04:53,545:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:53,561:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:53,668:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:53,668:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:53,962:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:54,393:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:04:54,571:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:54,969:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:55,244:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:55,303:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:04:55,425:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:55,661:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:04:55,856:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:08,372:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:08,409:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:08,559:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:08,702:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:09,494:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:09,540:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:09,687:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:09,783:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:09,929:INFO:Calculating mean and std
2025-05-05 08:05:09,934:INFO:Creating metrics dataframe
2025-05-05 08:05:09,941:INFO:Uploading results into container
2025-05-05 08:05:09,942:INFO:Uploading model into container now
2025-05-05 08:05:09,942:INFO:_master_model_container: 5
2025-05-05 08:05:09,942:INFO:_display_container: 2
2025-05-05 08:05:09,948:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6757, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-05-05 08:05:09,953:INFO:create_model() successfully completed......................................
2025-05-05 08:05:10,372:INFO:SubProcess create_model() end ==================================
2025-05-05 08:05:10,374:INFO:Creating metrics dataframe
2025-05-05 08:05:10,469:INFO:Initializing Ridge Classifier
2025-05-05 08:05:10,469:INFO:Total runtime is 3.8481366753578183 minutes
2025-05-05 08:05:10,483:INFO:SubProcess create_model() called ==================================
2025-05-05 08:05:10,483:INFO:Initializing create_model()
2025-05-05 08:05:10,483:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:05:10,483:INFO:Checking exceptions
2025-05-05 08:05:10,483:INFO:Importing libraries
2025-05-05 08:05:10,491:INFO:Copying training dataset
2025-05-05 08:05:10,637:INFO:Defining folds
2025-05-05 08:05:10,637:INFO:Declaring metric variables
2025-05-05 08:05:10,686:INFO:Importing untrained model
2025-05-05 08:05:10,717:INFO:Ridge Classifier Imported successfully
2025-05-05 08:05:10,807:INFO:Starting cross validation
2025-05-05 08:05:10,820:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:05:21,004:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:21,017:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:21,017:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:21,055:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:21,060:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,070:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,070:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,101:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,183:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,182:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,218:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,294:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,318:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,334:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,350:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:21,414:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,251:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:30,304:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,383:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:30,492:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,533:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,668:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:30,668:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:30,668:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,668:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,699:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,706:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,853:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,853:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,872:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:30,992:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:31,002:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:38,516:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:38,553:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:38,583:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:05:38,608:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:38,632:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:38,666:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:38,686:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:38,741:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:05:38,855:INFO:Calculating mean and std
2025-05-05 08:05:38,860:INFO:Creating metrics dataframe
2025-05-05 08:05:38,876:INFO:Uploading results into container
2025-05-05 08:05:38,884:INFO:Uploading model into container now
2025-05-05 08:05:38,884:INFO:_master_model_container: 6
2025-05-05 08:05:38,884:INFO:_display_container: 2
2025-05-05 08:05:38,888:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6757, solver='auto',
                tol=0.0001)
2025-05-05 08:05:38,889:INFO:create_model() successfully completed......................................
2025-05-05 08:05:39,181:INFO:SubProcess create_model() end ==================================
2025-05-05 08:05:39,181:INFO:Creating metrics dataframe
2025-05-05 08:05:39,216:INFO:Initializing Random Forest Classifier
2025-05-05 08:05:39,217:INFO:Total runtime is 4.32726324001948 minutes
2025-05-05 08:05:39,233:INFO:SubProcess create_model() called ==================================
2025-05-05 08:05:39,233:INFO:Initializing create_model()
2025-05-05 08:05:39,233:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:05:39,233:INFO:Checking exceptions
2025-05-05 08:05:39,233:INFO:Importing libraries
2025-05-05 08:05:39,233:INFO:Copying training dataset
2025-05-05 08:05:39,533:INFO:Defining folds
2025-05-05 08:05:39,533:INFO:Declaring metric variables
2025-05-05 08:05:39,558:INFO:Importing untrained model
2025-05-05 08:05:39,709:INFO:Random Forest Classifier Imported successfully
2025-05-05 08:05:39,854:INFO:Starting cross validation
2025-05-05 08:05:39,868:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:07:14,144:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,194:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,347:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,440:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,621:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,723:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,748:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,870:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,883:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:14,968:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:15,007:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:07:15,053:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:57,025:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:57,492:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:57,579:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:57,630:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:57,910:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:57,944:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:58,005:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:58,153:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:08:58,193:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:00,049:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:00,132:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:00,217:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:50,807:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:50,977:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:51,195:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:51,639:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:51,733:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:51,790:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:09:51,878:INFO:Calculating mean and std
2025-05-05 08:09:51,883:INFO:Creating metrics dataframe
2025-05-05 08:09:51,893:INFO:Uploading results into container
2025-05-05 08:09:51,896:INFO:Uploading model into container now
2025-05-05 08:09:51,897:INFO:_master_model_container: 7
2025-05-05 08:09:51,897:INFO:_display_container: 2
2025-05-05 08:09:51,899:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=6757, verbose=0,
                       warm_start=False)
2025-05-05 08:09:51,899:INFO:create_model() successfully completed......................................
2025-05-05 08:09:52,190:INFO:SubProcess create_model() end ==================================
2025-05-05 08:09:52,190:INFO:Creating metrics dataframe
2025-05-05 08:09:52,213:INFO:Initializing Quadratic Discriminant Analysis
2025-05-05 08:09:52,214:INFO:Total runtime is 8.5438872218132 minutes
2025-05-05 08:09:52,228:INFO:SubProcess create_model() called ==================================
2025-05-05 08:09:52,229:INFO:Initializing create_model()
2025-05-05 08:09:52,229:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:09:52,230:INFO:Checking exceptions
2025-05-05 08:09:52,230:INFO:Importing libraries
2025-05-05 08:09:52,231:INFO:Copying training dataset
2025-05-05 08:09:52,322:INFO:Defining folds
2025-05-05 08:09:52,322:INFO:Declaring metric variables
2025-05-05 08:09:52,341:INFO:Importing untrained model
2025-05-05 08:09:52,360:INFO:Quadratic Discriminant Analysis Imported successfully
2025-05-05 08:09:52,398:INFO:Starting cross validation
2025-05-05 08:09:52,407:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:10:00,470:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:00,840:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:00,857:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:01,038:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:01,108:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,304:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,385:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:01,468:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,528:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:01,555:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,560:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:01,592:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,620:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,707:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,747:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,862:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:01,910:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:01,933:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:02,007:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:02,096:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:06,233:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:06,763:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:06,792:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:06,792:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:06,884:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:07,059:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:07,152:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:07,344:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:07,628:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:07,678:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:07,811:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:07,823:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:07,840:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:07,894:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:07,912:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:07,922:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:07,944:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:07,987:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:08,040:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:08,128:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:10,503:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:10,767:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:10,783:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 08:10:10,790:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:10,847:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:10,890:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:10,997:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:11,009:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:11,059:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:11,109:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:11,200:INFO:Calculating mean and std
2025-05-05 08:10:11,203:INFO:Creating metrics dataframe
2025-05-05 08:10:11,210:INFO:Uploading results into container
2025-05-05 08:10:11,212:INFO:Uploading model into container now
2025-05-05 08:10:11,214:INFO:_master_model_container: 8
2025-05-05 08:10:11,214:INFO:_display_container: 2
2025-05-05 08:10:11,217:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-05-05 08:10:11,217:INFO:create_model() successfully completed......................................
2025-05-05 08:10:11,458:INFO:SubProcess create_model() end ==================================
2025-05-05 08:10:11,458:INFO:Creating metrics dataframe
2025-05-05 08:10:11,495:INFO:Initializing Ada Boost Classifier
2025-05-05 08:10:11,495:INFO:Total runtime is 8.865222462018329 minutes
2025-05-05 08:10:11,514:INFO:SubProcess create_model() called ==================================
2025-05-05 08:10:11,515:INFO:Initializing create_model()
2025-05-05 08:10:11,520:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:10:11,520:INFO:Checking exceptions
2025-05-05 08:10:11,521:INFO:Importing libraries
2025-05-05 08:10:11,521:INFO:Copying training dataset
2025-05-05 08:10:11,684:INFO:Defining folds
2025-05-05 08:10:11,684:INFO:Declaring metric variables
2025-05-05 08:10:11,734:INFO:Importing untrained model
2025-05-05 08:10:11,752:INFO:Ada Boost Classifier Imported successfully
2025-05-05 08:10:11,827:INFO:Starting cross validation
2025-05-05 08:10:11,830:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:10:16,628:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:10:16,628:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:10:16,717:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:10:41,822:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:41,830:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:41,847:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:41,852:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:41,917:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:41,917:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:41,971:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:41,974:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:42,292:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:42,301:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:10:42,320:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:42,330:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:42,378:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:42,382:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:42,439:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:42,441:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:10:46,120:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:10:46,187:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:10:46,657:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:10:46,714:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:11:12,411:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:11:12,442:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:12,524:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:12,527:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:11:12,565:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:12,588:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:12,679:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:12,821:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:12,900:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:11:12,924:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:13,007:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:13,020:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:11:13,043:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:13,075:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:13,105:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:13,166:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:15,808:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:11:15,887:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 08:11:33,329:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:11:33,347:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:33,401:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:33,454:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:33,457:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:11:33,469:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:33,509:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:33,539:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:11:33,597:INFO:Calculating mean and std
2025-05-05 08:11:33,600:INFO:Creating metrics dataframe
2025-05-05 08:11:33,605:INFO:Uploading results into container
2025-05-05 08:11:33,607:INFO:Uploading model into container now
2025-05-05 08:11:33,613:INFO:_master_model_container: 9
2025-05-05 08:11:33,613:INFO:_display_container: 2
2025-05-05 08:11:33,613:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=6757)
2025-05-05 08:11:33,613:INFO:create_model() successfully completed......................................
2025-05-05 08:11:33,963:INFO:SubProcess create_model() end ==================================
2025-05-05 08:11:33,964:INFO:Creating metrics dataframe
2025-05-05 08:11:33,987:INFO:Initializing Gradient Boosting Classifier
2025-05-05 08:11:33,988:INFO:Total runtime is 10.240106046199797 minutes
2025-05-05 08:11:34,001:INFO:SubProcess create_model() called ==================================
2025-05-05 08:11:34,002:INFO:Initializing create_model()
2025-05-05 08:11:34,003:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:11:34,003:INFO:Checking exceptions
2025-05-05 08:11:34,003:INFO:Importing libraries
2025-05-05 08:11:34,004:INFO:Copying training dataset
2025-05-05 08:11:34,181:INFO:Defining folds
2025-05-05 08:11:34,182:INFO:Declaring metric variables
2025-05-05 08:11:34,197:INFO:Importing untrained model
2025-05-05 08:11:34,217:INFO:Gradient Boosting Classifier Imported successfully
2025-05-05 08:11:34,257:INFO:Starting cross validation
2025-05-05 08:11:34,264:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:20:26,364:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:20:26,397:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:26,529:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:26,642:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:27,185:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:20:27,221:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:27,258:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:20:27,314:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:27,364:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:27,417:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:27,431:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:27,523:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:29,086:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:20:29,161:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:29,220:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:20:29,301:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:41,917:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:28:41,944:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:42,075:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:42,179:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:43,630:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:28:43,670:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:43,741:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:43,844:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:45,268:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:28:45,340:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:45,489:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:45,584:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:50,345:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:28:50,367:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:50,425:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:28:50,490:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:00,057:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:00,071:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:00,108:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:00,138:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:02,987:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:03,007:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:03,051:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:03,102:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:03,176:INFO:Calculating mean and std
2025-05-05 08:34:03,180:INFO:Creating metrics dataframe
2025-05-05 08:34:03,186:INFO:Uploading results into container
2025-05-05 08:34:03,188:INFO:Uploading model into container now
2025-05-05 08:34:03,190:INFO:_master_model_container: 10
2025-05-05 08:34:03,190:INFO:_display_container: 2
2025-05-05 08:34:03,191:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6757, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-05-05 08:34:03,194:INFO:create_model() successfully completed......................................
2025-05-05 08:34:03,448:INFO:SubProcess create_model() end ==================================
2025-05-05 08:34:03,448:INFO:Creating metrics dataframe
2025-05-05 08:34:03,472:INFO:Initializing Linear Discriminant Analysis
2025-05-05 08:34:03,472:INFO:Total runtime is 32.73151752154032 minutes
2025-05-05 08:34:03,482:INFO:SubProcess create_model() called ==================================
2025-05-05 08:34:03,484:INFO:Initializing create_model()
2025-05-05 08:34:03,484:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:34:03,485:INFO:Checking exceptions
2025-05-05 08:34:03,485:INFO:Importing libraries
2025-05-05 08:34:03,486:INFO:Copying training dataset
2025-05-05 08:34:03,532:INFO:Defining folds
2025-05-05 08:34:03,532:INFO:Declaring metric variables
2025-05-05 08:34:03,548:INFO:Importing untrained model
2025-05-05 08:34:03,562:INFO:Linear Discriminant Analysis Imported successfully
2025-05-05 08:34:03,589:INFO:Starting cross validation
2025-05-05 08:34:03,598:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:34:08,253:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:08,278:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:08,300:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:08,320:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:08,321:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:08,358:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:08,358:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:08,404:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:12,190:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:12,211:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:12,231:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:12,261:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:12,296:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:12,343:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:12,374:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:12,439:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:16,771:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:16,787:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:16,794:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:16,811:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:16,840:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:16,860:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:16,889:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:16,909:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:22,938:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:22,978:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:23,019:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:23,046:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:23,054:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:23,092:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:23,105:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:23,146:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:47,443:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 08:34:47,548:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:47,549:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:47,657:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:47,657:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:47,893:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:47,898:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:34:47,985:INFO:Calculating mean and std
2025-05-05 08:34:47,990:INFO:Creating metrics dataframe
2025-05-05 08:34:48,000:INFO:Uploading results into container
2025-05-05 08:34:48,001:INFO:Uploading model into container now
2025-05-05 08:34:48,003:INFO:_master_model_container: 11
2025-05-05 08:34:48,003:INFO:_display_container: 2
2025-05-05 08:34:48,004:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-05-05 08:34:48,005:INFO:create_model() successfully completed......................................
2025-05-05 08:34:48,377:INFO:SubProcess create_model() end ==================================
2025-05-05 08:34:48,377:INFO:Creating metrics dataframe
2025-05-05 08:34:48,434:INFO:Initializing Extra Trees Classifier
2025-05-05 08:34:48,434:INFO:Total runtime is 33.480884905656175 minutes
2025-05-05 08:34:48,450:INFO:SubProcess create_model() called ==================================
2025-05-05 08:34:48,451:INFO:Initializing create_model()
2025-05-05 08:34:48,452:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024A8CEF4820>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024A8CEE3CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 08:34:48,452:INFO:Checking exceptions
2025-05-05 08:34:48,452:INFO:Importing libraries
2025-05-05 08:34:48,452:INFO:Copying training dataset
2025-05-05 08:34:48,521:INFO:Defining folds
2025-05-05 08:34:48,522:INFO:Declaring metric variables
2025-05-05 08:34:48,536:INFO:Importing untrained model
2025-05-05 08:34:48,550:INFO:Extra Trees Classifier Imported successfully
2025-05-05 08:34:48,590:INFO:Starting cross validation
2025-05-05 08:34:48,598:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 08:40:56,590:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:03,699:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:03,716:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:04,328:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:04,400:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:04,828:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:05,110:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:05,233:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:13,347:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:20,969:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:41:21,399:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:51,327:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:51,739:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:51,813:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:52,002:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:52,409:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:52,727:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:54,533:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:55,723:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:55,860:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:58,154:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:58,371:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:43:58,615:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:44:18,249:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:44:18,306:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:44:18,405:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:44:26,293:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:44:26,351:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:44:26,408:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 08:44:42,524:INFO:Calculating mean and std
2025-05-05 08:44:46,110:INFO:Creating metrics dataframe
2025-05-05 08:44:47,594:INFO:Uploading results into container
2025-05-05 08:44:48,201:INFO:Uploading model into container now
2025-05-05 08:44:48,716:INFO:_master_model_container: 12
2025-05-05 08:44:48,717:INFO:_display_container: 2
2025-05-05 08:44:49,805:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=6757, verbose=0,
                     warm_start=False)
2025-05-05 08:44:49,805:INFO:create_model() successfully completed......................................
2025-05-05 11:35:25,514:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_380\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-05 11:35:35,852:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 11:35:35,852:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 11:35:35,852:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 11:35:35,852:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 11:35:48,952:INFO:PyCaret ClassificationExperiment
2025-05-05 11:35:48,952:INFO:Logging name: clf-default-name
2025-05-05 11:35:48,952:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-05 11:35:48,952:INFO:version 3.3.2
2025-05-05 11:35:48,952:INFO:Initializing setup()
2025-05-05 11:35:48,952:INFO:self.USI: af0e
2025-05-05 11:35:48,952:INFO:self._variable_keys: {'logging_param', '_available_plots', 'is_multiclass', '_ml_usecase', 'y', 'X_train', 'USI', 'fold_groups_param', 'data', 'gpu_n_jobs_param', 'exp_id', 'X', 'pipeline', 'fix_imbalance', 'log_plots_param', 'idx', 'fold_shuffle_param', 'target_param', 'y_test', 'seed', 'n_jobs_param', 'X_test', 'memory', 'html_param', 'fold_generator', 'exp_name_log', 'y_train', 'gpu_param'}
2025-05-05 11:35:48,952:INFO:Checking environment
2025-05-05 11:35:48,952:INFO:python_version: 3.10.11
2025-05-05 11:35:48,952:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-05 11:35:48,952:INFO:machine: AMD64
2025-05-05 11:35:48,952:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-05 11:35:48,962:INFO:Memory: svmem(total=8482373632, available=2343219200, percent=72.4, used=6139154432, free=2343219200)
2025-05-05 11:35:48,962:INFO:Physical Core: 2
2025-05-05 11:35:48,963:INFO:Logical Core: 4
2025-05-05 11:35:48,963:INFO:Checking libraries
2025-05-05 11:35:48,963:INFO:System:
2025-05-05 11:35:48,963:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-05 11:35:48,964:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-05 11:35:48,964:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-05 11:35:48,964:INFO:PyCaret required dependencies:
2025-05-05 11:35:50,486:INFO:                 pip: 25.0.1
2025-05-05 11:35:50,489:INFO:          setuptools: 65.5.0
2025-05-05 11:35:50,489:INFO:             pycaret: 3.3.2
2025-05-05 11:35:50,489:INFO:             IPython: 8.34.0
2025-05-05 11:35:50,489:INFO:          ipywidgets: 8.1.5
2025-05-05 11:35:50,490:INFO:                tqdm: 4.67.1
2025-05-05 11:35:50,490:INFO:               numpy: 1.26.4
2025-05-05 11:35:50,490:INFO:              pandas: 2.1.4
2025-05-05 11:35:50,490:INFO:              jinja2: 3.1.6
2025-05-05 11:35:50,490:INFO:               scipy: 1.11.4
2025-05-05 11:35:50,490:INFO:              joblib: 1.3.2
2025-05-05 11:35:50,493:INFO:             sklearn: 1.4.2
2025-05-05 11:35:50,497:INFO:                pyod: 2.0.4
2025-05-05 11:35:50,497:INFO:            imblearn: 0.13.0
2025-05-05 11:35:50,497:INFO:   category_encoders: 2.7.0
2025-05-05 11:35:50,498:INFO:            lightgbm: 4.6.0
2025-05-05 11:35:50,498:INFO:               numba: 0.61.0
2025-05-05 11:35:50,498:INFO:            requests: 2.32.3
2025-05-05 11:35:50,498:INFO:          matplotlib: 3.7.5
2025-05-05 11:35:50,498:INFO:          scikitplot: 0.3.7
2025-05-05 11:35:50,498:INFO:         yellowbrick: 1.5
2025-05-05 11:35:50,498:INFO:              plotly: 5.24.1
2025-05-05 11:35:50,498:INFO:    plotly-resampler: Not installed
2025-05-05 11:35:50,498:INFO:             kaleido: 0.2.1
2025-05-05 11:35:50,498:INFO:           schemdraw: 0.15
2025-05-05 11:35:50,498:INFO:         statsmodels: 0.14.4
2025-05-05 11:35:50,498:INFO:              sktime: 0.26.0
2025-05-05 11:35:50,498:INFO:               tbats: 1.1.3
2025-05-05 11:35:50,498:INFO:            pmdarima: 2.0.4
2025-05-05 11:35:50,498:INFO:              psutil: 7.0.0
2025-05-05 11:35:50,498:INFO:          markupsafe: 3.0.2
2025-05-05 11:35:50,502:INFO:             pickle5: Not installed
2025-05-05 11:35:50,502:INFO:         cloudpickle: 3.1.1
2025-05-05 11:35:50,502:INFO:         deprecation: 2.1.0
2025-05-05 11:35:50,503:INFO:              xxhash: 3.5.0
2025-05-05 11:35:50,503:INFO:           wurlitzer: Not installed
2025-05-05 11:35:50,503:INFO:PyCaret optional dependencies:
2025-05-05 11:35:50,586:INFO:                shap: 0.44.1
2025-05-05 11:35:50,586:INFO:           interpret: 0.6.10
2025-05-05 11:35:50,586:INFO:                umap: 0.5.7
2025-05-05 11:35:50,586:INFO:     ydata_profiling: 4.16.1
2025-05-05 11:35:50,586:INFO:  explainerdashboard: 0.4.8
2025-05-05 11:35:50,587:INFO:             autoviz: Not installed
2025-05-05 11:35:50,587:INFO:           fairlearn: 0.7.0
2025-05-05 11:35:50,587:INFO:          deepchecks: Not installed
2025-05-05 11:35:50,587:INFO:             xgboost: Not installed
2025-05-05 11:35:50,587:INFO:            catboost: Not installed
2025-05-05 11:35:50,587:INFO:              kmodes: Not installed
2025-05-05 11:35:50,587:INFO:             mlxtend: Not installed
2025-05-05 11:35:50,587:INFO:       statsforecast: Not installed
2025-05-05 11:35:50,587:INFO:        tune_sklearn: Not installed
2025-05-05 11:35:50,587:INFO:                 ray: Not installed
2025-05-05 11:35:50,588:INFO:            hyperopt: Not installed
2025-05-05 11:35:50,588:INFO:              optuna: 4.2.1
2025-05-05 11:35:50,588:INFO:               skopt: Not installed
2025-05-05 11:35:50,588:INFO:              mlflow: Not installed
2025-05-05 11:35:50,588:INFO:              gradio: Not installed
2025-05-05 11:35:50,588:INFO:             fastapi: Not installed
2025-05-05 11:35:50,588:INFO:             uvicorn: Not installed
2025-05-05 11:35:50,588:INFO:              m2cgen: Not installed
2025-05-05 11:35:50,588:INFO:           evidently: Not installed
2025-05-05 11:35:50,588:INFO:               fugue: Not installed
2025-05-05 11:35:50,588:INFO:           streamlit: Not installed
2025-05-05 11:35:50,589:INFO:             prophet: Not installed
2025-05-05 11:35:50,589:INFO:None
2025-05-05 11:35:50,589:INFO:Set up data.
2025-05-05 11:35:50,638:INFO:Set up folding strategy.
2025-05-05 11:35:50,638:INFO:Set up train/test split.
2025-05-05 11:35:50,967:INFO:Set up index.
2025-05-05 11:35:50,985:INFO:Assigning column types.
2025-05-05 11:35:51,066:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-05 11:35:51,295:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 11:35:51,323:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 11:35:51,485:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:51,485:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:51,840:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 11:35:51,848:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 11:35:51,937:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:51,938:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:51,938:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-05 11:35:52,023:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 11:35:52,087:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:52,087:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:52,196:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 11:35:52,247:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:52,247:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:52,247:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-05 11:35:52,515:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:52,515:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:52,748:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:52,748:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:52,763:INFO:Preparing preprocessing pipeline...
2025-05-05 11:35:52,764:INFO:Set up label encoding.
2025-05-05 11:35:52,764:INFO:Set up simple imputation.
2025-05-05 11:35:52,781:INFO:Set up encoding of categorical features.
2025-05-05 11:35:52,781:INFO:Set up imbalanced handling.
2025-05-05 11:35:52,781:INFO:Set up feature normalization.
2025-05-05 11:35:56,598:INFO:Finished creating preprocessing pipeline.
2025-05-05 11:35:56,630:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'depth', 'table', 'price',
                                             'x', 'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-05 11:35:56,630:INFO:Creating final display dataframe.
2025-05-05 11:35:59,715:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16                Fix imbalance   
17         Fix imbalance method   
18                    Normalize   
19             Normalize method   
20               Fold Generator   
21                  Fold Number   
22                     CPU Jobs   
23                      Use GPU   
24               Log Experiment   
25              Experiment Name   
26                          USI   

                                                Value  
0                                                5657  
1                                                 cut  
2                                          Multiclass  
3   Fair: 0, Good: 1, Ideal: 2, Premium: 3, Very G...  
4                                         (53940, 10)  
5                                         (96993, 23)  
6                                         (86205, 23)  
7                                         (10788, 23)  
8                                                   7  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                               True  
17                                              smote  
18                                               True  
19                                             minmax  
20                                    StratifiedKFold  
21                                                 10  
22                                                 -1  
23                                              False  
24                                              False  
25                                   clf-default-name  
26                                               af0e  
2025-05-05 11:35:59,995:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:35:59,995:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:36:00,199:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:36:00,212:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 11:36:00,213:INFO:setup() successfully completed in 13.14s...............
2025-05-05 11:36:00,247:INFO:Initializing compare_models()
2025-05-05 11:36:00,247:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-05-05 11:36:00,247:INFO:Checking exceptions
2025-05-05 11:36:00,287:INFO:Preparing display monitor
2025-05-05 11:36:00,434:INFO:Initializing Logistic Regression
2025-05-05 11:36:00,434:INFO:Total runtime is 0.0 minutes
2025-05-05 11:36:00,456:INFO:SubProcess create_model() called ==================================
2025-05-05 11:36:00,457:INFO:Initializing create_model()
2025-05-05 11:36:00,457:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:36:00,457:INFO:Checking exceptions
2025-05-05 11:36:00,457:INFO:Importing libraries
2025-05-05 11:36:00,457:INFO:Copying training dataset
2025-05-05 11:36:00,713:INFO:Defining folds
2025-05-05 11:36:00,713:INFO:Declaring metric variables
2025-05-05 11:36:00,767:INFO:Importing untrained model
2025-05-05 11:36:00,783:INFO:Logistic Regression Imported successfully
2025-05-05 11:36:00,832:INFO:Starting cross validation
2025-05-05 11:36:00,837:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:36:33,339:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:36:33,396:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:33,462:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:33,462:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:33,526:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:33,526:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:36:33,526:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:33,543:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:33,618:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:33,696:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:34,367:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:36:34,411:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:34,484:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:34,543:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:49,371:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:36:49,392:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:49,460:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:49,527:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:49,609:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:36:49,642:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:49,708:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:49,775:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:49,995:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:36:50,035:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:50,092:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:50,166:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:51,081:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:36:51,102:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:51,163:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:36:51,212:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:02,291:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:37:02,311:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:02,376:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:02,411:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:02,595:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:37:02,644:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:02,678:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:02,712:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:02,792:INFO:Calculating mean and std
2025-05-05 11:37:02,795:INFO:Creating metrics dataframe
2025-05-05 11:37:02,813:INFO:Uploading results into container
2025-05-05 11:37:02,813:INFO:Uploading model into container now
2025-05-05 11:37:02,821:INFO:_master_model_container: 1
2025-05-05 11:37:02,821:INFO:_display_container: 2
2025-05-05 11:37:02,822:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5657, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-05 11:37:02,824:INFO:create_model() successfully completed......................................
2025-05-05 11:37:02,962:INFO:SubProcess create_model() end ==================================
2025-05-05 11:37:02,962:INFO:Creating metrics dataframe
2025-05-05 11:37:02,999:INFO:Initializing K Neighbors Classifier
2025-05-05 11:37:02,999:INFO:Total runtime is 1.0427531679471334 minutes
2025-05-05 11:37:03,021:INFO:SubProcess create_model() called ==================================
2025-05-05 11:37:03,023:INFO:Initializing create_model()
2025-05-05 11:37:03,023:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:37:03,024:INFO:Checking exceptions
2025-05-05 11:37:03,025:INFO:Importing libraries
2025-05-05 11:37:03,025:INFO:Copying training dataset
2025-05-05 11:37:03,250:INFO:Defining folds
2025-05-05 11:37:03,252:INFO:Declaring metric variables
2025-05-05 11:37:03,270:INFO:Importing untrained model
2025-05-05 11:37:03,301:INFO:K Neighbors Classifier Imported successfully
2025-05-05 11:37:03,340:INFO:Starting cross validation
2025-05-05 11:37:03,352:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:37:17,077:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,091:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,143:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,143:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,158:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,177:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,191:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,191:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,207:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,230:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,259:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:17,294:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:30,858:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:30,858:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:30,874:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:30,874:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:30,951:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:30,952:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:30,957:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:30,979:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:31,007:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:31,007:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:31,024:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:31,024:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:39,842:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:39,875:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:39,875:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:39,909:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:39,909:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:39,941:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:40,024:INFO:Calculating mean and std
2025-05-05 11:37:40,027:INFO:Creating metrics dataframe
2025-05-05 11:37:40,050:INFO:Uploading results into container
2025-05-05 11:37:40,056:INFO:Uploading model into container now
2025-05-05 11:37:40,062:INFO:_master_model_container: 2
2025-05-05 11:37:40,062:INFO:_display_container: 2
2025-05-05 11:37:40,065:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-05-05 11:37:40,065:INFO:create_model() successfully completed......................................
2025-05-05 11:37:40,278:INFO:SubProcess create_model() end ==================================
2025-05-05 11:37:40,278:INFO:Creating metrics dataframe
2025-05-05 11:37:40,316:INFO:Initializing Naive Bayes
2025-05-05 11:37:40,317:INFO:Total runtime is 1.6647210558255514 minutes
2025-05-05 11:37:40,345:INFO:SubProcess create_model() called ==================================
2025-05-05 11:37:40,347:INFO:Initializing create_model()
2025-05-05 11:37:40,348:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:37:40,349:INFO:Checking exceptions
2025-05-05 11:37:40,350:INFO:Importing libraries
2025-05-05 11:37:40,350:INFO:Copying training dataset
2025-05-05 11:37:40,500:INFO:Defining folds
2025-05-05 11:37:40,501:INFO:Declaring metric variables
2025-05-05 11:37:40,516:INFO:Importing untrained model
2025-05-05 11:37:40,535:INFO:Naive Bayes Imported successfully
2025-05-05 11:37:40,574:INFO:Starting cross validation
2025-05-05 11:37:40,584:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:37:45,173:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,195:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,205:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,232:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,248:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,264:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,274:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,290:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,312:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,340:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,340:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:45,396:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,615:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,615:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,662:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,677:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,677:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,693:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,728:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,728:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,743:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,759:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,775:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:49,816:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:52,641:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:52,657:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:52,675:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:52,692:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:52,714:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:52,725:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:37:52,791:INFO:Calculating mean and std
2025-05-05 11:37:52,793:INFO:Creating metrics dataframe
2025-05-05 11:37:52,812:INFO:Uploading results into container
2025-05-05 11:37:52,815:INFO:Uploading model into container now
2025-05-05 11:37:52,815:INFO:_master_model_container: 3
2025-05-05 11:37:52,815:INFO:_display_container: 2
2025-05-05 11:37:52,815:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-05-05 11:37:52,815:INFO:create_model() successfully completed......................................
2025-05-05 11:37:52,942:INFO:SubProcess create_model() end ==================================
2025-05-05 11:37:52,942:INFO:Creating metrics dataframe
2025-05-05 11:37:52,974:INFO:Initializing Decision Tree Classifier
2025-05-05 11:37:52,974:INFO:Total runtime is 1.8756786624590556 minutes
2025-05-05 11:37:52,984:INFO:SubProcess create_model() called ==================================
2025-05-05 11:37:52,985:INFO:Initializing create_model()
2025-05-05 11:37:52,986:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:37:52,986:INFO:Checking exceptions
2025-05-05 11:37:52,987:INFO:Importing libraries
2025-05-05 11:37:52,987:INFO:Copying training dataset
2025-05-05 11:37:53,066:INFO:Defining folds
2025-05-05 11:37:53,067:INFO:Declaring metric variables
2025-05-05 11:37:53,084:INFO:Importing untrained model
2025-05-05 11:37:53,124:INFO:Decision Tree Classifier Imported successfully
2025-05-05 11:37:53,195:INFO:Starting cross validation
2025-05-05 11:37:53,206:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:38:02,912:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:02,929:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:02,956:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:02,973:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:02,988:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:03,007:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:03,042:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:03,072:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:03,072:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:03,224:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:03,290:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:03,378:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:11,712:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:11,774:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:11,811:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:11,842:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:11,872:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:11,890:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:11,943:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:11,958:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:12,024:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:12,129:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:12,190:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:12,256:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:18,873:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:18,941:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:18,958:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:18,977:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:18,994:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:19,023:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:19,072:INFO:Calculating mean and std
2025-05-05 11:38:19,074:INFO:Creating metrics dataframe
2025-05-05 11:38:19,081:INFO:Uploading results into container
2025-05-05 11:38:19,082:INFO:Uploading model into container now
2025-05-05 11:38:19,084:INFO:_master_model_container: 4
2025-05-05 11:38:19,084:INFO:_display_container: 2
2025-05-05 11:38:19,088:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=5657, splitter='best')
2025-05-05 11:38:19,091:INFO:create_model() successfully completed......................................
2025-05-05 11:38:19,223:INFO:SubProcess create_model() end ==================================
2025-05-05 11:38:19,223:INFO:Creating metrics dataframe
2025-05-05 11:38:19,264:INFO:Initializing SVM - Linear Kernel
2025-05-05 11:38:19,266:INFO:Total runtime is 2.3138704697291055 minutes
2025-05-05 11:38:19,281:INFO:SubProcess create_model() called ==================================
2025-05-05 11:38:19,284:INFO:Initializing create_model()
2025-05-05 11:38:19,285:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:38:19,285:INFO:Checking exceptions
2025-05-05 11:38:19,285:INFO:Importing libraries
2025-05-05 11:38:19,285:INFO:Copying training dataset
2025-05-05 11:38:19,406:INFO:Defining folds
2025-05-05 11:38:19,406:INFO:Declaring metric variables
2025-05-05 11:38:19,429:INFO:Importing untrained model
2025-05-05 11:38:19,458:INFO:SVM - Linear Kernel Imported successfully
2025-05-05 11:38:19,493:INFO:Starting cross validation
2025-05-05 11:38:19,505:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:38:26,363:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:26,409:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:26,411:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:26,455:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:26,626:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:26,633:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:26,649:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:26,691:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:26,691:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:26,733:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:26,762:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:26,837:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:27,106:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:27,122:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:27,212:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:27,293:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:32,895:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:32,971:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:33,105:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:33,263:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:33,574:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:33,621:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:33,639:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:33,654:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:33,729:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:33,754:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:33,838:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:33,887:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:34,312:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:34,340:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:34,374:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:34,440:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:37,839:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:37,857:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:37,920:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:37,973:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:38,341:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:38,356:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:38,393:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:38,422:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:38,487:INFO:Calculating mean and std
2025-05-05 11:38:38,490:INFO:Creating metrics dataframe
2025-05-05 11:38:38,495:INFO:Uploading results into container
2025-05-05 11:38:38,497:INFO:Uploading model into container now
2025-05-05 11:38:38,498:INFO:_master_model_container: 5
2025-05-05 11:38:38,498:INFO:_display_container: 2
2025-05-05 11:38:38,499:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5657, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-05-05 11:38:38,499:INFO:create_model() successfully completed......................................
2025-05-05 11:38:38,629:INFO:SubProcess create_model() end ==================================
2025-05-05 11:38:38,630:INFO:Creating metrics dataframe
2025-05-05 11:38:38,680:INFO:Initializing Ridge Classifier
2025-05-05 11:38:38,680:INFO:Total runtime is 2.6374429504076637 minutes
2025-05-05 11:38:38,695:INFO:SubProcess create_model() called ==================================
2025-05-05 11:38:38,695:INFO:Initializing create_model()
2025-05-05 11:38:38,696:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:38:38,696:INFO:Checking exceptions
2025-05-05 11:38:38,696:INFO:Importing libraries
2025-05-05 11:38:38,697:INFO:Copying training dataset
2025-05-05 11:38:38,769:INFO:Defining folds
2025-05-05 11:38:38,770:INFO:Declaring metric variables
2025-05-05 11:38:38,787:INFO:Importing untrained model
2025-05-05 11:38:38,803:INFO:Ridge Classifier Imported successfully
2025-05-05 11:38:38,848:INFO:Starting cross validation
2025-05-05 11:38:38,859:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:38:43,955:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:43,955:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:43,970:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:43,970:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:43,987:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:43,987:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:43,995:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,004:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,054:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,054:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,054:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,069:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,106:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,106:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,155:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:44,156:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,288:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:48,304:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:48,323:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:48,337:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,338:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,354:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,377:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:48,394:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,403:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,403:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,403:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,453:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,453:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,453:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,470:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:48,503:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:51,314:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:51,322:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:38:51,342:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:51,356:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:51,387:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:51,396:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:51,431:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:51,439:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:38:51,520:INFO:Calculating mean and std
2025-05-05 11:38:51,523:INFO:Creating metrics dataframe
2025-05-05 11:38:51,529:INFO:Uploading results into container
2025-05-05 11:38:51,530:INFO:Uploading model into container now
2025-05-05 11:38:51,531:INFO:_master_model_container: 6
2025-05-05 11:38:51,532:INFO:_display_container: 2
2025-05-05 11:38:51,533:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5657, solver='auto',
                tol=0.0001)
2025-05-05 11:38:51,535:INFO:create_model() successfully completed......................................
2025-05-05 11:38:51,674:INFO:SubProcess create_model() end ==================================
2025-05-05 11:38:51,674:INFO:Creating metrics dataframe
2025-05-05 11:38:51,703:INFO:Initializing Random Forest Classifier
2025-05-05 11:38:51,704:INFO:Total runtime is 2.8545007785161336 minutes
2025-05-05 11:38:51,724:INFO:SubProcess create_model() called ==================================
2025-05-05 11:38:51,725:INFO:Initializing create_model()
2025-05-05 11:38:51,726:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:38:51,726:INFO:Checking exceptions
2025-05-05 11:38:51,727:INFO:Importing libraries
2025-05-05 11:38:51,727:INFO:Copying training dataset
2025-05-05 11:38:51,849:INFO:Defining folds
2025-05-05 11:38:51,849:INFO:Declaring metric variables
2025-05-05 11:38:51,882:INFO:Importing untrained model
2025-05-05 11:38:51,908:INFO:Random Forest Classifier Imported successfully
2025-05-05 11:38:51,943:INFO:Starting cross validation
2025-05-05 11:38:51,955:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:40:00,922:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:01,263:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:01,404:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:01,475:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:01,488:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:01,518:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:01,566:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:01,582:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:01,635:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:02,067:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:02,184:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:40:02,257:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:10,517:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:10,649:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:10,649:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:10,851:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:10,867:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:10,913:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:10,913:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:11,086:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:11,216:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:11,580:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:11,648:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:11,717:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:46,581:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:46,665:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:46,825:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:47,119:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:47,164:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:47,216:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:47,312:INFO:Calculating mean and std
2025-05-05 11:41:47,319:INFO:Creating metrics dataframe
2025-05-05 11:41:47,336:INFO:Uploading results into container
2025-05-05 11:41:47,338:INFO:Uploading model into container now
2025-05-05 11:41:47,341:INFO:_master_model_container: 7
2025-05-05 11:41:47,341:INFO:_display_container: 2
2025-05-05 11:41:47,343:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=5657, verbose=0,
                       warm_start=False)
2025-05-05 11:41:47,346:INFO:create_model() successfully completed......................................
2025-05-05 11:41:47,500:INFO:SubProcess create_model() end ==================================
2025-05-05 11:41:47,500:INFO:Creating metrics dataframe
2025-05-05 11:41:47,540:INFO:Initializing Quadratic Discriminant Analysis
2025-05-05 11:41:47,540:INFO:Total runtime is 5.7851143956184385 minutes
2025-05-05 11:41:47,557:INFO:SubProcess create_model() called ==================================
2025-05-05 11:41:47,557:INFO:Initializing create_model()
2025-05-05 11:41:47,558:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:41:47,558:INFO:Checking exceptions
2025-05-05 11:41:47,559:INFO:Importing libraries
2025-05-05 11:41:47,559:INFO:Copying training dataset
2025-05-05 11:41:47,770:INFO:Defining folds
2025-05-05 11:41:47,771:INFO:Declaring metric variables
2025-05-05 11:41:47,791:INFO:Importing untrained model
2025-05-05 11:41:47,816:INFO:Quadratic Discriminant Analysis Imported successfully
2025-05-05 11:41:47,859:INFO:Starting cross validation
2025-05-05 11:41:47,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:41:52,452:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 11:41:52,929:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:41:52,945:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:41:52,954:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:41:52,954:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:41:52,962:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:52,978:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:52,978:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:52,978:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:53,027:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:53,035:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:53,035:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:53,044:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:53,078:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:53,078:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:53,097:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:56,897:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 11:41:56,930:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 11:41:57,046:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 11:41:57,054:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 11:41:57,444:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:41:57,462:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:41:57,462:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,478:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:41:57,478:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,511:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,511:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:41:57,529:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,529:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,551:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,561:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,594:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,594:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,610:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,628:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:41:57,652:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:00,670:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 11:42:00,747:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-05-05 11:42:00,964:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:00,998:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:01,042:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:01,065:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:01,072:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:01,097:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:01,097:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:01,152:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:01,211:INFO:Calculating mean and std
2025-05-05 11:42:01,214:INFO:Creating metrics dataframe
2025-05-05 11:42:01,218:INFO:Uploading results into container
2025-05-05 11:42:01,219:INFO:Uploading model into container now
2025-05-05 11:42:01,220:INFO:_master_model_container: 8
2025-05-05 11:42:01,221:INFO:_display_container: 2
2025-05-05 11:42:01,223:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-05-05 11:42:01,224:INFO:create_model() successfully completed......................................
2025-05-05 11:42:01,368:INFO:SubProcess create_model() end ==================================
2025-05-05 11:42:01,369:INFO:Creating metrics dataframe
2025-05-05 11:42:01,403:INFO:Initializing Ada Boost Classifier
2025-05-05 11:42:01,403:INFO:Total runtime is 6.0161601305007935 minutes
2025-05-05 11:42:01,416:INFO:SubProcess create_model() called ==================================
2025-05-05 11:42:01,417:INFO:Initializing create_model()
2025-05-05 11:42:01,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:42:01,418:INFO:Checking exceptions
2025-05-05 11:42:01,418:INFO:Importing libraries
2025-05-05 11:42:01,418:INFO:Copying training dataset
2025-05-05 11:42:01,460:INFO:Defining folds
2025-05-05 11:42:01,461:INFO:Declaring metric variables
2025-05-05 11:42:01,472:INFO:Importing untrained model
2025-05-05 11:42:01,485:INFO:Ada Boost Classifier Imported successfully
2025-05-05 11:42:01,507:INFO:Starting cross validation
2025-05-05 11:42:01,514:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:42:05,796:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:05,837:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:05,845:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:05,958:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:24,543:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:24,560:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,634:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,644:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:24,676:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,694:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,694:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:24,726:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:24,726:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,734:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,745:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,783:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,810:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,813:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,913:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:24,931:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:28,559:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:28,795:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:28,913:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:28,978:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:46,993:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:47,034:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,109:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,192:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,258:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:47,276:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,370:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,478:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,593:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:47,626:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,702:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,725:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:42:47,742:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,759:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,816:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:47,893:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:42:50,790:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:42:50,909:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-05 11:43:04,394:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:43:04,411:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:43:04,453:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:43:04,494:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:43:04,611:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:43:04,645:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:43:04,744:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:43:04,832:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:43:04,910:INFO:Calculating mean and std
2025-05-05 11:43:04,917:INFO:Creating metrics dataframe
2025-05-05 11:43:04,930:INFO:Uploading results into container
2025-05-05 11:43:04,933:INFO:Uploading model into container now
2025-05-05 11:43:04,936:INFO:_master_model_container: 9
2025-05-05 11:43:04,937:INFO:_display_container: 2
2025-05-05 11:43:04,941:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=5657)
2025-05-05 11:43:04,941:INFO:create_model() successfully completed......................................
2025-05-05 11:43:05,146:INFO:SubProcess create_model() end ==================================
2025-05-05 11:43:05,146:INFO:Creating metrics dataframe
2025-05-05 11:43:05,215:INFO:Initializing Gradient Boosting Classifier
2025-05-05 11:43:05,215:INFO:Total runtime is 7.079682469367981 minutes
2025-05-05 11:43:05,230:INFO:SubProcess create_model() called ==================================
2025-05-05 11:43:05,231:INFO:Initializing create_model()
2025-05-05 11:43:05,232:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001DABC346650>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001DAC0BE2710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-05 11:43:05,232:INFO:Checking exceptions
2025-05-05 11:43:05,233:INFO:Importing libraries
2025-05-05 11:43:05,233:INFO:Copying training dataset
2025-05-05 11:43:05,336:INFO:Defining folds
2025-05-05 11:43:05,336:INFO:Declaring metric variables
2025-05-05 11:43:05,357:INFO:Importing untrained model
2025-05-05 11:43:05,382:INFO:Gradient Boosting Classifier Imported successfully
2025-05-05 11:43:05,424:INFO:Starting cross validation
2025-05-05 11:43:05,433:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-05 11:49:14,124:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:49:14,141:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:14,208:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:14,274:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:15,641:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:49:15,641:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:49:15,674:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:15,675:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:15,731:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:15,769:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:15,812:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:15,824:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:17,708:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-05 11:49:17,741:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:17,808:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 11:49:17,876:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\metrics\_classification.py:1561: UserWarning: Note that pos_label (set to 'Very Good') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2025-05-05 12:03:13,361:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 12:03:13,361:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 12:03:13,361:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 12:03:13,361:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-05 12:03:44,353:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_10464\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-05 12:03:48,632:INFO:PyCaret ClassificationExperiment
2025-05-05 12:03:48,634:INFO:Logging name: clf-default-name
2025-05-05 12:03:48,634:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-05 12:03:48,634:INFO:version 3.3.2
2025-05-05 12:03:48,634:INFO:Initializing setup()
2025-05-05 12:03:48,634:INFO:self.USI: 193b
2025-05-05 12:03:48,634:INFO:self._variable_keys: {'_available_plots', 'USI', 'seed', 'X_test', 'n_jobs_param', 'gpu_n_jobs_param', 'is_multiclass', 'fold_groups_param', 'log_plots_param', 'fix_imbalance', 'y_test', 'html_param', 'y_train', 'logging_param', 'pipeline', 'exp_name_log', 'gpu_param', 'exp_id', 'fold_generator', 'memory', 'idx', 'y', 'target_param', 'X', 'X_train', '_ml_usecase', 'fold_shuffle_param', 'data'}
2025-05-05 12:03:48,634:INFO:Checking environment
2025-05-05 12:03:48,634:INFO:python_version: 3.10.11
2025-05-05 12:03:48,634:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-05 12:03:48,634:INFO:machine: AMD64
2025-05-05 12:03:48,634:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-05 12:03:48,634:INFO:Memory: svmem(total=8482373632, available=2691645440, percent=68.3, used=5790728192, free=2691645440)
2025-05-05 12:03:48,634:INFO:Physical Core: 2
2025-05-05 12:03:48,634:INFO:Logical Core: 4
2025-05-05 12:03:48,642:INFO:Checking libraries
2025-05-05 12:03:48,642:INFO:System:
2025-05-05 12:03:48,642:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-05 12:03:48,642:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-05 12:03:48,642:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-05 12:03:48,642:INFO:PyCaret required dependencies:
2025-05-05 12:03:48,946:INFO:                 pip: 25.0.1
2025-05-05 12:03:48,948:INFO:          setuptools: 65.5.0
2025-05-05 12:03:48,948:INFO:             pycaret: 3.3.2
2025-05-05 12:03:48,948:INFO:             IPython: 8.34.0
2025-05-05 12:03:48,948:INFO:          ipywidgets: 8.1.5
2025-05-05 12:03:48,948:INFO:                tqdm: 4.67.1
2025-05-05 12:03:48,948:INFO:               numpy: 1.26.4
2025-05-05 12:03:48,948:INFO:              pandas: 2.1.4
2025-05-05 12:03:48,948:INFO:              jinja2: 3.1.6
2025-05-05 12:03:48,948:INFO:               scipy: 1.11.4
2025-05-05 12:03:48,948:INFO:              joblib: 1.3.2
2025-05-05 12:03:48,948:INFO:             sklearn: 1.4.2
2025-05-05 12:03:48,949:INFO:                pyod: 2.0.4
2025-05-05 12:03:48,949:INFO:            imblearn: 0.13.0
2025-05-05 12:03:48,949:INFO:   category_encoders: 2.7.0
2025-05-05 12:03:48,949:INFO:            lightgbm: 4.6.0
2025-05-05 12:03:48,949:INFO:               numba: 0.61.0
2025-05-05 12:03:48,949:INFO:            requests: 2.32.3
2025-05-05 12:03:48,949:INFO:          matplotlib: 3.7.5
2025-05-05 12:03:48,949:INFO:          scikitplot: 0.3.7
2025-05-05 12:03:48,949:INFO:         yellowbrick: 1.5
2025-05-05 12:03:48,950:INFO:              plotly: 5.24.1
2025-05-05 12:03:48,950:INFO:    plotly-resampler: Not installed
2025-05-05 12:03:48,951:INFO:             kaleido: 0.2.1
2025-05-05 12:03:48,951:INFO:           schemdraw: 0.15
2025-05-05 12:03:48,951:INFO:         statsmodels: 0.14.4
2025-05-05 12:03:48,951:INFO:              sktime: 0.26.0
2025-05-05 12:03:48,952:INFO:               tbats: 1.1.3
2025-05-05 12:03:48,952:INFO:            pmdarima: 2.0.4
2025-05-05 12:03:48,952:INFO:              psutil: 7.0.0
2025-05-05 12:03:48,952:INFO:          markupsafe: 3.0.2
2025-05-05 12:03:48,952:INFO:             pickle5: Not installed
2025-05-05 12:03:48,952:INFO:         cloudpickle: 3.1.1
2025-05-05 12:03:48,952:INFO:         deprecation: 2.1.0
2025-05-05 12:03:48,953:INFO:              xxhash: 3.5.0
2025-05-05 12:03:48,953:INFO:           wurlitzer: Not installed
2025-05-05 12:03:48,953:INFO:PyCaret optional dependencies:
2025-05-05 12:03:48,986:INFO:                shap: 0.44.1
2025-05-05 12:03:48,986:INFO:           interpret: 0.6.10
2025-05-05 12:03:48,986:INFO:                umap: 0.5.7
2025-05-05 12:03:48,986:INFO:     ydata_profiling: 4.16.1
2025-05-05 12:03:48,986:INFO:  explainerdashboard: 0.4.8
2025-05-05 12:03:48,986:INFO:             autoviz: Not installed
2025-05-05 12:03:48,986:INFO:           fairlearn: 0.7.0
2025-05-05 12:03:48,986:INFO:          deepchecks: Not installed
2025-05-05 12:03:48,986:INFO:             xgboost: Not installed
2025-05-05 12:03:48,986:INFO:            catboost: Not installed
2025-05-05 12:03:48,986:INFO:              kmodes: Not installed
2025-05-05 12:03:48,986:INFO:             mlxtend: Not installed
2025-05-05 12:03:48,986:INFO:       statsforecast: Not installed
2025-05-05 12:03:48,986:INFO:        tune_sklearn: Not installed
2025-05-05 12:03:48,986:INFO:                 ray: Not installed
2025-05-05 12:03:48,986:INFO:            hyperopt: Not installed
2025-05-05 12:03:48,986:INFO:              optuna: 4.2.1
2025-05-05 12:03:48,986:INFO:               skopt: Not installed
2025-05-05 12:03:48,986:INFO:              mlflow: Not installed
2025-05-05 12:03:48,986:INFO:              gradio: Not installed
2025-05-05 12:03:48,986:INFO:             fastapi: Not installed
2025-05-05 12:03:48,986:INFO:             uvicorn: Not installed
2025-05-05 12:03:48,986:INFO:              m2cgen: Not installed
2025-05-05 12:03:48,986:INFO:           evidently: Not installed
2025-05-05 12:03:48,986:INFO:               fugue: Not installed
2025-05-05 12:03:48,986:INFO:           streamlit: Not installed
2025-05-05 12:03:48,986:INFO:             prophet: Not installed
2025-05-05 12:03:48,986:INFO:None
2025-05-05 12:03:48,986:INFO:Set up data.
2025-05-05 12:03:49,036:INFO:Set up folding strategy.
2025-05-05 12:03:49,036:INFO:Set up train/test split.
2025-05-05 12:03:49,172:INFO:Set up index.
2025-05-05 12:03:49,172:INFO:Assigning column types.
2025-05-05 12:03:49,203:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-05 12:03:49,374:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 12:03:49,386:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 12:03:49,470:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:49,470:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:49,607:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 12:03:49,609:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 12:03:49,703:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:49,703:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:49,703:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-05 12:03:49,802:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 12:03:49,963:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:49,964:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:50,104:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 12:03:50,170:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:50,170:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:50,170:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-05 12:03:50,353:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:50,353:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:50,654:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:50,654:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:50,670:INFO:Preparing preprocessing pipeline...
2025-05-05 12:03:50,677:INFO:Set up label encoding.
2025-05-05 12:03:50,677:INFO:Set up simple imputation.
2025-05-05 12:03:50,692:INFO:Set up encoding of categorical features.
2025-05-05 12:03:50,692:INFO:Set up imbalanced handling.
2025-05-05 12:03:50,692:INFO:Set up feature normalization.
2025-05-05 12:03:53,585:INFO:Finished creating preprocessing pipeline.
2025-05-05 12:03:53,603:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'depth', 'table', 'price',
                                             'x', 'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-05 12:03:53,603:INFO:Creating final display dataframe.
2025-05-05 12:03:56,415:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16                Fix imbalance   
17         Fix imbalance method   
18                    Normalize   
19             Normalize method   
20               Fold Generator   
21                  Fold Number   
22                     CPU Jobs   
23                      Use GPU   
24               Log Experiment   
25              Experiment Name   
26                          USI   

                                                Value  
0                                                3928  
1                                                 cut  
2                                          Multiclass  
3   Fair: 0, Good: 1, Ideal: 2, Premium: 3, Very G...  
4                                         (53940, 10)  
5                                         (96993, 23)  
6                                         (86205, 23)  
7                                         (10788, 23)  
8                                                   7  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                               True  
17                                              smote  
18                                               True  
19                                             minmax  
20                                    StratifiedKFold  
21                                                 10  
22                                                 -1  
23                                              False  
24                                              False  
25                                   clf-default-name  
26                                               193b  
2025-05-05 12:03:56,670:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:56,670:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:56,870:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:56,871:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:03:56,872:INFO:setup() successfully completed in 8.7s...............
2025-05-05 12:16:21,330:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_10464\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-05 12:16:24,576:INFO:PyCaret ClassificationExperiment
2025-05-05 12:16:24,576:INFO:Logging name: clf-default-name
2025-05-05 12:16:24,576:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-05 12:16:24,576:INFO:version 3.3.2
2025-05-05 12:16:24,576:INFO:Initializing setup()
2025-05-05 12:16:24,576:INFO:self.USI: 9736
2025-05-05 12:16:24,576:INFO:self._variable_keys: {'_available_plots', 'USI', 'seed', 'X_test', 'n_jobs_param', 'gpu_n_jobs_param', 'is_multiclass', 'fold_groups_param', 'log_plots_param', 'fix_imbalance', 'y_test', 'html_param', 'y_train', 'logging_param', 'pipeline', 'exp_name_log', 'gpu_param', 'exp_id', 'fold_generator', 'memory', 'idx', 'y', 'target_param', 'X', 'X_train', '_ml_usecase', 'fold_shuffle_param', 'data'}
2025-05-05 12:16:24,576:INFO:Checking environment
2025-05-05 12:16:24,576:INFO:python_version: 3.10.11
2025-05-05 12:16:24,576:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-05 12:16:24,576:INFO:machine: AMD64
2025-05-05 12:16:24,576:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-05 12:16:24,576:INFO:Memory: svmem(total=8482373632, available=1740009472, percent=79.5, used=6742364160, free=1740009472)
2025-05-05 12:16:24,576:INFO:Physical Core: 2
2025-05-05 12:16:24,576:INFO:Logical Core: 4
2025-05-05 12:16:24,576:INFO:Checking libraries
2025-05-05 12:16:24,576:INFO:System:
2025-05-05 12:16:24,576:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-05 12:16:24,576:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-05 12:16:24,576:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-05 12:16:24,576:INFO:PyCaret required dependencies:
2025-05-05 12:16:24,576:INFO:                 pip: 25.0.1
2025-05-05 12:16:24,576:INFO:          setuptools: 65.5.0
2025-05-05 12:16:24,576:INFO:             pycaret: 3.3.2
2025-05-05 12:16:24,576:INFO:             IPython: 8.34.0
2025-05-05 12:16:24,576:INFO:          ipywidgets: 8.1.5
2025-05-05 12:16:24,576:INFO:                tqdm: 4.67.1
2025-05-05 12:16:24,576:INFO:               numpy: 1.26.4
2025-05-05 12:16:24,592:INFO:              pandas: 2.1.4
2025-05-05 12:16:24,592:INFO:              jinja2: 3.1.6
2025-05-05 12:16:24,592:INFO:               scipy: 1.11.4
2025-05-05 12:16:24,592:INFO:              joblib: 1.3.2
2025-05-05 12:16:24,592:INFO:             sklearn: 1.4.2
2025-05-05 12:16:24,592:INFO:                pyod: 2.0.4
2025-05-05 12:16:24,592:INFO:            imblearn: 0.13.0
2025-05-05 12:16:24,592:INFO:   category_encoders: 2.7.0
2025-05-05 12:16:24,592:INFO:            lightgbm: 4.6.0
2025-05-05 12:16:24,592:INFO:               numba: 0.61.0
2025-05-05 12:16:24,592:INFO:            requests: 2.32.3
2025-05-05 12:16:24,592:INFO:          matplotlib: 3.7.5
2025-05-05 12:16:24,592:INFO:          scikitplot: 0.3.7
2025-05-05 12:16:24,592:INFO:         yellowbrick: 1.5
2025-05-05 12:16:24,592:INFO:              plotly: 5.24.1
2025-05-05 12:16:24,592:INFO:    plotly-resampler: Not installed
2025-05-05 12:16:24,592:INFO:             kaleido: 0.2.1
2025-05-05 12:16:24,592:INFO:           schemdraw: 0.15
2025-05-05 12:16:24,592:INFO:         statsmodels: 0.14.4
2025-05-05 12:16:24,592:INFO:              sktime: 0.26.0
2025-05-05 12:16:24,592:INFO:               tbats: 1.1.3
2025-05-05 12:16:24,592:INFO:            pmdarima: 2.0.4
2025-05-05 12:16:24,592:INFO:              psutil: 7.0.0
2025-05-05 12:16:24,592:INFO:          markupsafe: 3.0.2
2025-05-05 12:16:24,592:INFO:             pickle5: Not installed
2025-05-05 12:16:24,592:INFO:         cloudpickle: 3.1.1
2025-05-05 12:16:24,592:INFO:         deprecation: 2.1.0
2025-05-05 12:16:24,592:INFO:              xxhash: 3.5.0
2025-05-05 12:16:24,592:INFO:           wurlitzer: Not installed
2025-05-05 12:16:24,592:INFO:PyCaret optional dependencies:
2025-05-05 12:16:24,592:INFO:                shap: 0.44.1
2025-05-05 12:16:24,636:INFO:           interpret: 0.6.10
2025-05-05 12:16:24,636:INFO:                umap: 0.5.7
2025-05-05 12:16:24,636:INFO:     ydata_profiling: 4.16.1
2025-05-05 12:16:24,636:INFO:  explainerdashboard: 0.4.8
2025-05-05 12:16:24,636:INFO:             autoviz: Not installed
2025-05-05 12:16:24,636:INFO:           fairlearn: 0.7.0
2025-05-05 12:16:24,636:INFO:          deepchecks: Not installed
2025-05-05 12:16:24,636:INFO:             xgboost: Not installed
2025-05-05 12:16:24,636:INFO:            catboost: Not installed
2025-05-05 12:16:24,636:INFO:              kmodes: Not installed
2025-05-05 12:16:24,636:INFO:             mlxtend: Not installed
2025-05-05 12:16:24,636:INFO:       statsforecast: Not installed
2025-05-05 12:16:24,636:INFO:        tune_sklearn: Not installed
2025-05-05 12:16:24,636:INFO:                 ray: Not installed
2025-05-05 12:16:24,636:INFO:            hyperopt: Not installed
2025-05-05 12:16:24,636:INFO:              optuna: 4.2.1
2025-05-05 12:16:24,636:INFO:               skopt: Not installed
2025-05-05 12:16:24,636:INFO:              mlflow: Not installed
2025-05-05 12:16:24,636:INFO:              gradio: Not installed
2025-05-05 12:16:24,636:INFO:             fastapi: Not installed
2025-05-05 12:16:24,636:INFO:             uvicorn: Not installed
2025-05-05 12:16:24,636:INFO:              m2cgen: Not installed
2025-05-05 12:16:24,636:INFO:           evidently: Not installed
2025-05-05 12:16:24,636:INFO:               fugue: Not installed
2025-05-05 12:16:24,652:INFO:           streamlit: Not installed
2025-05-05 12:16:24,652:INFO:             prophet: Not installed
2025-05-05 12:16:24,652:INFO:None
2025-05-05 12:16:24,652:INFO:Set up data.
2025-05-05 12:16:24,716:INFO:Set up folding strategy.
2025-05-05 12:16:24,716:INFO:Set up train/test split.
2025-05-05 12:16:24,976:INFO:Set up index.
2025-05-05 12:16:24,976:INFO:Assigning column types.
2025-05-05 12:16:25,001:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-05 12:16:25,090:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 12:16:25,099:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 12:16:25,135:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,135:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,217:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-05 12:16:25,217:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 12:16:25,268:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,268:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,268:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-05 12:16:25,371:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 12:16:25,434:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,434:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,518:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-05 12:16:25,568:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,568:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,568:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-05 12:16:25,701:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,701:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,818:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,818:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:25,818:INFO:Preparing preprocessing pipeline...
2025-05-05 12:16:25,834:INFO:Set up label encoding.
2025-05-05 12:16:25,834:INFO:Set up simple imputation.
2025-05-05 12:16:25,850:INFO:Set up encoding of categorical features.
2025-05-05 12:16:25,850:INFO:Set up imbalanced handling.
2025-05-05 12:16:25,850:INFO:Set up feature normalization.
2025-05-05 12:16:28,684:INFO:Finished creating preprocessing pipeline.
2025-05-05 12:16:28,701:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'depth', 'table', 'price',
                                             'x', 'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-05 12:16:28,701:INFO:Creating final display dataframe.
2025-05-05 12:16:31,335:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16                Fix imbalance   
17         Fix imbalance method   
18                    Normalize   
19             Normalize method   
20               Fold Generator   
21                  Fold Number   
22                     CPU Jobs   
23                      Use GPU   
24               Log Experiment   
25              Experiment Name   
26                          USI   

                                                Value  
0                                                5041  
1                                                 cut  
2                                          Multiclass  
3   Fair: 0, Good: 1, Ideal: 2, Premium: 3, Very G...  
4                                         (53940, 10)  
5                                         (96993, 23)  
6                                         (86205, 23)  
7                                         (10788, 23)  
8                                                   7  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                               True  
17                                              smote  
18                                               True  
19                                             minmax  
20                                    StratifiedKFold  
21                                                 10  
22                                                 -1  
23                                              False  
24                                              False  
25                                   clf-default-name  
26                                               9736  
2025-05-05 12:16:31,551:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:31,551:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:31,733:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:31,733:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-05 12:16:31,734:INFO:setup() successfully completed in 7.56s...............
2025-05-05 12:28:34,299:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:37,003:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:39,527:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:43,485:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:46,071:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:51,622:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:53,784:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:55,740:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:57,734:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:28:59,550:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:02,067:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:03,949:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:05,887:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:07,934:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:09,732:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:12,000:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:13,699:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:15,400:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:17,495:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:19,517:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:29:19,551:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
25 fits failed out of a total of 50.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
25 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\pipeline.py", line 475, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py", line 1172, in fit
    solver = _check_solver(self.solver, self.penalty, self.dual)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py", line 67, in _check_solver
    raise ValueError(
ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-05-05 12:29:19,551:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.62166305        nan 0.64738604        nan 0.65227573
        nan 0.65257701        nan 0.65218299]
  warnings.warn(

2025-05-05 12:29:22,182:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:12,127:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:15,467:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:20,959:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:24,591:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:26,988:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:29,926:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:32,784:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:35,100:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:37,287:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:38,854:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:41,059:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:45,825:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:52,871:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:54,764:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:32:57,175:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:33:02,346:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:33:04,822:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:33:07,058:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:33:09,306:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:33:11,228:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:33:11,254:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
25 fits failed out of a total of 50.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
25 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\pipeline.py", line 475, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py", line 1172, in fit
    solver = _check_solver(self.solver, self.penalty, self.dual)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py", line 67, in _check_solver
    raise ValueError(
ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-05-05 12:33:11,270:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.62166305        nan 0.64738604        nan 0.65227573
        nan 0.65257701        nan 0.65218299]
  warnings.warn(

2025-05-05 12:33:13,867:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:03,067:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:05,385:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:08,651:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:12,134:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:18,024:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:20,795:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:22,873:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:24,851:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:28,226:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:30,141:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:31,862:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:33,296:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:34,704:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:36,112:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:37,594:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:39,262:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:40,672:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:42,038:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:43,531:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:44,863:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-05 12:35:44,880:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
25 fits failed out of a total of 50.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
25 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\pipeline.py", line 475, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py", line 1172, in fit
    solver = _check_solver(self.solver, self.penalty, self.dual)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py", line 67, in _check_solver
    raise ValueError(
ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-05-05 12:35:44,880:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.62166305        nan 0.64738604        nan 0.65227573
        nan 0.65257701        nan 0.65218299]
  warnings.warn(

2025-05-05 12:35:46,594:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:24:42,976:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 11:24:54,612:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-07 11:24:54,613:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-07 11:24:54,614:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-07 11:24:54,614:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-07 11:25:20,669:INFO:PyCaret ClassificationExperiment
2025-05-07 11:25:20,670:INFO:Logging name: clf-default-name
2025-05-07 11:25:20,670:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-07 11:25:20,671:INFO:version 3.3.2
2025-05-07 11:25:20,671:INFO:Initializing setup()
2025-05-07 11:25:20,672:INFO:self.USI: 991a
2025-05-07 11:25:20,674:INFO:self._variable_keys: {'memory', 'pipeline', 'exp_name_log', 'fold_shuffle_param', '_available_plots', 'fold_generator', 'seed', 'exp_id', 'gpu_n_jobs_param', 'USI', 'fix_imbalance', 'gpu_param', 'y', 'is_multiclass', 'data', 'y_test', 'logging_param', 'X_train', 'fold_groups_param', 'log_plots_param', 'idx', 'html_param', '_ml_usecase', 'target_param', 'X', 'y_train', 'n_jobs_param', 'X_test'}
2025-05-07 11:25:20,674:INFO:Checking environment
2025-05-07 11:25:20,675:INFO:python_version: 3.10.11
2025-05-07 11:25:20,677:INFO:python_build: ('tags/v3.10.11:7d4cc5a', 'Apr  5 2023 00:38:17')
2025-05-07 11:25:20,677:INFO:machine: AMD64
2025-05-07 11:25:20,678:INFO:platform: Windows-10-10.0.19045-SP0
2025-05-07 11:25:20,689:INFO:Memory: svmem(total=8482373632, available=1768620032, percent=79.1, used=6713753600, free=1768620032)
2025-05-07 11:25:20,689:INFO:Physical Core: 2
2025-05-07 11:25:20,689:INFO:Logical Core: 4
2025-05-07 11:25:20,694:INFO:Checking libraries
2025-05-07 11:25:20,695:INFO:System:
2025-05-07 11:25:20,695:INFO:    python: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]
2025-05-07 11:25:20,695:INFO:executable: c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\Scripts\python.exe
2025-05-07 11:25:20,695:INFO:   machine: Windows-10-10.0.19045-SP0
2025-05-07 11:25:20,695:INFO:PyCaret required dependencies:
2025-05-07 11:25:22,212:INFO:                 pip: 25.0.1
2025-05-07 11:25:22,212:INFO:          setuptools: 65.5.0
2025-05-07 11:25:22,212:INFO:             pycaret: 3.3.2
2025-05-07 11:25:22,212:INFO:             IPython: 8.34.0
2025-05-07 11:25:22,212:INFO:          ipywidgets: 8.1.5
2025-05-07 11:25:22,212:INFO:                tqdm: 4.67.1
2025-05-07 11:25:22,212:INFO:               numpy: 1.26.4
2025-05-07 11:25:22,212:INFO:              pandas: 2.1.4
2025-05-07 11:25:22,212:INFO:              jinja2: 3.1.6
2025-05-07 11:25:22,212:INFO:               scipy: 1.11.4
2025-05-07 11:25:22,212:INFO:              joblib: 1.3.2
2025-05-07 11:25:22,212:INFO:             sklearn: 1.4.2
2025-05-07 11:25:22,212:INFO:                pyod: 2.0.4
2025-05-07 11:25:22,212:INFO:            imblearn: 0.13.0
2025-05-07 11:25:22,212:INFO:   category_encoders: 2.7.0
2025-05-07 11:25:22,212:INFO:            lightgbm: 4.6.0
2025-05-07 11:25:22,212:INFO:               numba: 0.61.0
2025-05-07 11:25:22,212:INFO:            requests: 2.32.3
2025-05-07 11:25:22,212:INFO:          matplotlib: 3.7.5
2025-05-07 11:25:22,212:INFO:          scikitplot: 0.3.7
2025-05-07 11:25:22,225:INFO:         yellowbrick: 1.5
2025-05-07 11:25:22,225:INFO:              plotly: 5.24.1
2025-05-07 11:25:22,225:INFO:    plotly-resampler: Not installed
2025-05-07 11:25:22,226:INFO:             kaleido: 0.2.1
2025-05-07 11:25:22,226:INFO:           schemdraw: 0.15
2025-05-07 11:25:22,226:INFO:         statsmodels: 0.14.4
2025-05-07 11:25:22,226:INFO:              sktime: 0.26.0
2025-05-07 11:25:22,226:INFO:               tbats: 1.1.3
2025-05-07 11:25:22,226:INFO:            pmdarima: 2.0.4
2025-05-07 11:25:22,226:INFO:              psutil: 7.0.0
2025-05-07 11:25:22,226:INFO:          markupsafe: 3.0.2
2025-05-07 11:25:22,226:INFO:             pickle5: Not installed
2025-05-07 11:25:22,226:INFO:         cloudpickle: 3.1.1
2025-05-07 11:25:22,226:INFO:         deprecation: 2.1.0
2025-05-07 11:25:22,226:INFO:              xxhash: 3.5.0
2025-05-07 11:25:22,226:INFO:           wurlitzer: Not installed
2025-05-07 11:25:22,226:INFO:PyCaret optional dependencies:
2025-05-07 11:25:22,363:INFO:                shap: 0.44.1
2025-05-07 11:25:22,363:INFO:           interpret: 0.6.10
2025-05-07 11:25:22,363:INFO:                umap: 0.5.7
2025-05-07 11:25:22,363:INFO:     ydata_profiling: 4.16.1
2025-05-07 11:25:22,363:INFO:  explainerdashboard: 0.4.8
2025-05-07 11:25:22,363:INFO:             autoviz: Not installed
2025-05-07 11:25:22,363:INFO:           fairlearn: 0.7.0
2025-05-07 11:25:22,363:INFO:          deepchecks: Not installed
2025-05-07 11:25:22,366:INFO:             xgboost: Not installed
2025-05-07 11:25:22,366:INFO:            catboost: Not installed
2025-05-07 11:25:22,367:INFO:              kmodes: Not installed
2025-05-07 11:25:22,367:INFO:             mlxtend: Not installed
2025-05-07 11:25:22,367:INFO:       statsforecast: Not installed
2025-05-07 11:25:22,367:INFO:        tune_sklearn: Not installed
2025-05-07 11:25:22,367:INFO:                 ray: Not installed
2025-05-07 11:25:22,367:INFO:            hyperopt: Not installed
2025-05-07 11:25:22,367:INFO:              optuna: 4.2.1
2025-05-07 11:25:22,367:INFO:               skopt: Not installed
2025-05-07 11:25:22,367:INFO:              mlflow: Not installed
2025-05-07 11:25:22,367:INFO:              gradio: Not installed
2025-05-07 11:25:22,367:INFO:             fastapi: Not installed
2025-05-07 11:25:22,367:INFO:             uvicorn: Not installed
2025-05-07 11:25:22,367:INFO:              m2cgen: Not installed
2025-05-07 11:25:22,367:INFO:           evidently: Not installed
2025-05-07 11:25:22,367:INFO:               fugue: Not installed
2025-05-07 11:25:22,367:INFO:           streamlit: Not installed
2025-05-07 11:25:22,367:INFO:             prophet: Not installed
2025-05-07 11:25:22,367:INFO:None
2025-05-07 11:25:22,367:INFO:Set up data.
2025-05-07 11:25:22,511:INFO:Set up folding strategy.
2025-05-07 11:25:22,511:INFO:Set up train/test split.
2025-05-07 11:25:22,994:INFO:Set up index.
2025-05-07 11:25:23,009:INFO:Assigning column types.
2025-05-07 11:25:23,060:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-07 11:25:23,293:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-07 11:25:23,328:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-07 11:25:23,632:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:23,643:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:23,911:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-07 11:25:23,911:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-07 11:25:24,060:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:24,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:24,076:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-07 11:25:24,406:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-07 11:25:24,642:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:24,642:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:24,758:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-07 11:25:24,809:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:24,809:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:24,809:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-07 11:25:25,054:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:25,058:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:25,277:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:25,277:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:25,327:INFO:Preparing preprocessing pipeline...
2025-05-07 11:25:25,327:INFO:Set up label encoding.
2025-05-07 11:25:25,327:INFO:Set up simple imputation.
2025-05-07 11:25:25,378:INFO:Set up encoding of categorical features.
2025-05-07 11:25:25,378:INFO:Set up imbalanced handling.
2025-05-07 11:25:25,378:INFO:Set up feature normalization.
2025-05-07 11:25:30,159:INFO:Finished creating preprocessing pipeline.
2025-05-07 11:25:30,175:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\SANTI\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['carat', 'depth', 'table', 'price',
                                             'x', 'y', 'z'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('balance',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=FixImbalancer(estimator=SMOTE(k_neighbors=5,
                                                                              random_state=None,
                                                                              sampling_strategy='auto')))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=MinMaxScaler(clip=False,
                                                             copy=True,
                                                             feature_range=(0,
                                                                            1))))],
         verbose=False)
2025-05-07 11:25:30,175:INFO:Creating final display dataframe.
2025-05-07 11:25:32,791:INFO:Setup _display_container:                     Description  \
0                    Session id   
1                        Target   
2                   Target type   
3                Target mapping   
4           Original data shape   
5        Transformed data shape   
6   Transformed train set shape   
7    Transformed test set shape   
8              Numeric features   
9          Categorical features   
10                   Preprocess   
11              Imputation type   
12           Numeric imputation   
13       Categorical imputation   
14     Maximum one-hot encoding   
15              Encoding method   
16                Fix imbalance   
17         Fix imbalance method   
18                    Normalize   
19             Normalize method   
20               Fold Generator   
21                  Fold Number   
22                     CPU Jobs   
23                      Use GPU   
24               Log Experiment   
25              Experiment Name   
26                          USI   

                                                Value  
0                                                4008  
1                                                 cut  
2                                          Multiclass  
3   Fair: 0, Good: 1, Ideal: 2, Premium: 3, Very G...  
4                                         (53940, 10)  
5                                         (96993, 23)  
6                                         (86205, 23)  
7                                         (10788, 23)  
8                                                   7  
9                                                   2  
10                                               True  
11                                             simple  
12                                               mean  
13                                               mode  
14                                                 25  
15                                               None  
16                                               True  
17                                              smote  
18                                               True  
19                                             minmax  
20                                    StratifiedKFold  
21                                                 10  
22                                                 -1  
23                                              False  
24                                              False  
25                                   clf-default-name  
26                                               991a  
2025-05-07 11:25:33,073:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:33,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:33,191:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:33,191:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-07 11:25:33,191:INFO:setup() successfully completed in 15.86s...............
2025-05-07 11:25:44,107:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:25:47,457:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:25:49,641:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:25:51,455:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:25:53,328:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:25:55,673:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:25:57,572:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:25:59,472:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:01,475:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:03,359:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:05,605:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:07,710:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:09,778:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:11,741:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:13,807:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:16,089:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:18,140:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:20,138:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:22,055:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:23,846:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:26:23,870:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_validation.py:547: FitFailedWarning: 
25 fits failed out of a total of 50.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
25 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_validation.py", line 895, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\pipeline.py", line 475, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\base.py", line 1474, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py", line 1172, in fit
    solver = _check_solver(self.solver, self.penalty, self.dual)
  File "c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py", line 67, in _check_solver
    raise ValueError(
ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2025-05-07 11:26:23,980:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\model_selection\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.62166305        nan 0.64738604        nan 0.65227573
        nan 0.65257701        nan 0.65218299]
  warnings.warn(

2025-05-07 11:26:26,205:WARNING:c:\Users\SANTI\Downloads\Virtual_Environments\pycaret\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2025-05-07 11:50:54,839:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 12:04:18,551:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 12:25:07,275:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 12:31:28,071:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 12:35:27,520:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 12:35:58,911:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 13:01:47,887:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 14:28:34,702:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 17:39:28,621:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

2025-05-07 17:56:40,046:WARNING:C:\Users\SANTI\AppData\Local\Temp\ipykernel_6332\4162361693.py:1: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(x="color", y="price", data=data, palette='viridis')

